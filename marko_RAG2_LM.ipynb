{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f152be64c294e2f8d62b787735cede8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "27259830"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import datasets\n",
    "\n",
    "\n",
    "# R_NUMBER_SEED = 1234567 # Replace this with your own student number\n",
    "R_NUMBER_SEED = 928036 # my student number \n",
    "DOCS_TO_ADD = 1000\n",
    "query_documents = datasets.load_dataset(\"parquet\", data_files=\"./acl_anthology_queries.parquet\")[\"train\"]\n",
    "all_documents = datasets.load_dataset(\"parquet\", data_files=\"./acl_anthology_full.parquet\")[\"train\"]\n",
    "# Shuffle with seed and take only n docs\n",
    "shuffled_documents = all_documents.shuffle(seed=R_NUMBER_SEED)\n",
    "random_documents = shuffled_documents.select(range(DOCS_TO_ADD))\n",
    "# Concatenate relevant documents with random sample and shuffle again\n",
    "anthology_sample = datasets.concatenate_datasets([query_documents, random_documents]).shuffle(seed=R_NUMBER_SEED)\n",
    "# Export to Parquet to avoid downloading full anthology\n",
    "anthology_sample.to_parquet(\"./anthology_sample.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "queries = json.load(open(\"./acl_anthology_queries.json\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mjova\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sumy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "# def remove_stopwords(doc):\n",
    "#     text = ' '.join([word for word in doc.split() if word.lower() not in stop_words])\n",
    "\n",
    "# def preprocess_document(doc):\n",
    "#     # Flatten dictionary and combine relevant text fields\n",
    "#     text = f\"{doc.get('title', '')} {doc.get('abstract', '')} {doc.get('full_text', '')}\"\n",
    "    \n",
    "#     # Remove stopwords\n",
    "#     text = ' '.join([word for word in text.split() if word.lower() not in stop_words])\n",
    "    \n",
    "#     return text\n",
    "\n",
    "# def preprocess_documents(documents):\n",
    "#     return [preprocess_document(doc) for doc in documents]\n",
    "\n",
    "# preprocessed_documents = preprocess_documents(anthology_sample)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mjova\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc: 10\n",
      "doc: 20\n",
      "doc: 30\n",
      "doc: 40\n",
      "doc: 50\n",
      "doc: 60\n",
      "doc: 70\n",
      "doc: 80\n",
      "doc: 90\n",
      "doc: 100\n",
      "doc: 110\n",
      "doc: 120\n",
      "doc: 130\n",
      "doc: 140\n",
      "doc: 150\n",
      "doc: 160\n",
      "doc: 170\n",
      "doc: 180\n",
      "doc: 190\n",
      "doc: 200\n",
      "doc: 210\n",
      "doc: 220\n",
      "doc: 230\n",
      "doc: 240\n",
      "doc: 250\n",
      "doc: 260\n",
      "doc: 270\n",
      "doc: 280\n",
      "doc: 290\n",
      "doc: 300\n",
      "doc: 310\n",
      "doc: 320\n",
      "doc: 330\n",
      "doc: 340\n",
      "doc: 350\n",
      "doc: 360\n",
      "doc: 370\n",
      "doc: 380\n",
      "doc: 390\n",
      "doc: 400\n",
      "doc: 410\n",
      "doc: 420\n",
      "doc: 430\n",
      "doc: 440\n",
      "doc: 450\n",
      "doc: 460\n",
      "doc: 470\n",
      "doc: 480\n",
      "doc: 490\n",
      "doc: 500\n",
      "doc: 510\n",
      "doc: 520\n",
      "doc: 530\n",
      "doc: 540\n",
      "doc: 550\n",
      "doc: 560\n",
      "doc: 570\n",
      "doc: 580\n",
      "doc: 590\n",
      "doc: 600\n",
      "doc: 610\n",
      "doc: 620\n",
      "doc: 630\n",
      "doc: 640\n",
      "doc: 650\n",
      "doc: 660\n",
      "doc: 670\n",
      "doc: 680\n",
      "doc: 690\n",
      "doc: 700\n",
      "doc: 710\n",
      "doc: 720\n",
      "doc: 730\n",
      "doc: 740\n",
      "doc: 750\n",
      "doc: 760\n",
      "doc: 770\n",
      "doc: 780\n",
      "doc: 790\n",
      "doc: 800\n",
      "doc: 810\n",
      "doc: 820\n",
      "doc: 830\n",
      "doc: 840\n",
      "doc: 850\n",
      "doc: 860\n",
      "doc: 870\n",
      "doc: 880\n",
      "doc: 890\n",
      "doc: 900\n",
      "doc: 910\n",
      "doc: 920\n",
      "doc: 930\n",
      "doc: 940\n",
      "doc: 950\n",
      "doc: 960\n",
      "doc: 970\n",
      "doc: 980\n",
      "doc: 990\n",
      "doc: 1000\n",
      "doc: 1010\n",
      "doc: 1020\n",
      "doc: 1030\n",
      "doc: 1040\n",
      "doc: 1050\n",
      "doc: 1060\n",
      "doc: 1070\n",
      "doc: 1080\n",
      "doc: 1090\n",
      "doc: 1100\n",
      "doc: 1110\n",
      "doc: 1120\n",
      "doc: 1130\n",
      "doc: 1140\n",
      "doc: 1150\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def preprocess_documents_nltk(documents):\n",
    "    all_sentences = []\n",
    "    all_sentences_to_doc_map = []\n",
    "    count = 1\n",
    "    for doc in documents:\n",
    "        if count % 10 == 0:\n",
    "            print(\"doc:\",count)\n",
    "        # print(doc[\"acl_id\"])\n",
    "        # full_text = f\"{doc.get('full_text')}\"\n",
    "        full_text = \"\"\n",
    "        if doc[\"title\"] is not None: full_text = f'{doc[\"title\"]}.' \n",
    "        if doc[\"abstract\"] is not None: full_text = f'{full_text} {doc[\"abstract\"]}.' \n",
    "        if doc[\"full_text\"] is not None: full_text = f'{full_text} {doc[\"full_text\"]}.' \n",
    "        sentences = sent_tokenize(full_text)\n",
    "        for sentence in sentences:\n",
    "            all_sentences.append(sentence)\n",
    "            all_sentences_to_doc_map.append(doc[\"acl_id\"])\n",
    "        count += 1\n",
    "    return all_sentences,all_sentences_to_doc_map\n",
    "preprocessed_documents, sentence_to_doc_map = preprocess_documents_nltk(anthology_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize lists to store embeddings\n",
    "# Initialize the MiniLM model for document embeddings\n",
    "minilm_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "# Function to get document embedding using MiniLM\n",
    "def get_document_embedding(document):\n",
    "    return minilm_model.encode(document, convert_to_tensor=False)\n",
    "\n",
    "minilm_embeddings = []\n",
    "# bert_embeddings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192757"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preprocessed_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[139], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m document \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(document)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# print(\"Processing sentence embeddings:\",count)\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m indexes\u001b[38;5;241m.\u001b[39mappend(\u001b[43mdocument\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43macl_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     11\u001b[0m count\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": [
    "# Compute embeddings for the first 1000 documents\n",
    "indexes = []\n",
    "count = 1\n",
    "for document in preprocessed_documents[:10000]:\n",
    "    if count % 100 == 0:\n",
    "        print(\"Processing doc:\",count)\n",
    "    # Ensure the document is a string\n",
    "    document = str(document)\n",
    "    # print(\"Processing sentence embeddings:\",count)\n",
    "    indexes.append(document[\"acl_id\"])\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing doc: 100\n",
      "Processing doc: 200\n",
      "Processing doc: 300\n",
      "Processing doc: 400\n",
      "Processing doc: 500\n",
      "Processing doc: 600\n",
      "Processing doc: 700\n",
      "Processing doc: 800\n",
      "Processing doc: 900\n",
      "Processing doc: 1000\n",
      "Processing doc: 1100\n",
      "Processing doc: 1200\n",
      "Processing doc: 1300\n",
      "Processing doc: 1400\n",
      "Processing doc: 1500\n",
      "Processing doc: 1600\n",
      "Processing doc: 1700\n",
      "Processing doc: 1800\n",
      "Processing doc: 1900\n",
      "Processing doc: 2000\n",
      "Processing doc: 2100\n",
      "Processing doc: 2200\n",
      "Processing doc: 2300\n",
      "Processing doc: 2400\n",
      "Processing doc: 2500\n",
      "Processing doc: 2600\n",
      "Processing doc: 2700\n",
      "Processing doc: 2800\n",
      "Processing doc: 2900\n",
      "Processing doc: 3000\n",
      "Processing doc: 3100\n",
      "Processing doc: 3200\n",
      "Processing doc: 3300\n",
      "Processing doc: 3400\n",
      "Processing doc: 3500\n",
      "Processing doc: 3600\n",
      "Processing doc: 3700\n",
      "Processing doc: 3800\n",
      "Processing doc: 3900\n",
      "Processing doc: 4000\n",
      "Processing doc: 4100\n",
      "Processing doc: 4200\n",
      "Processing doc: 4300\n",
      "Processing doc: 4400\n",
      "Processing doc: 4500\n",
      "Processing doc: 4600\n",
      "Processing doc: 4700\n",
      "Processing doc: 4800\n",
      "Processing doc: 4900\n",
      "Processing doc: 5000\n",
      "Processing doc: 5100\n",
      "Processing doc: 5200\n",
      "Processing doc: 5300\n",
      "Processing doc: 5400\n",
      "Processing doc: 5500\n",
      "Processing doc: 5600\n",
      "Processing doc: 5700\n",
      "Processing doc: 5800\n",
      "Processing doc: 5900\n",
      "Processing doc: 6000\n",
      "Processing doc: 6100\n",
      "Processing doc: 6200\n",
      "Processing doc: 6300\n",
      "Processing doc: 6400\n",
      "Processing doc: 6500\n",
      "Processing doc: 6600\n",
      "Processing doc: 6700\n",
      "Processing doc: 6800\n",
      "Processing doc: 6900\n",
      "Processing doc: 7000\n",
      "Processing doc: 7100\n",
      "Processing doc: 7200\n",
      "Processing doc: 7300\n",
      "Processing doc: 7400\n",
      "Processing doc: 7500\n",
      "Processing doc: 7600\n",
      "Processing doc: 7700\n",
      "Processing doc: 7800\n",
      "Processing doc: 7900\n",
      "Processing doc: 8000\n",
      "Processing doc: 8100\n",
      "Processing doc: 8200\n",
      "Processing doc: 8300\n",
      "Processing doc: 8400\n",
      "Processing doc: 8500\n",
      "Processing doc: 8600\n",
      "Processing doc: 8700\n",
      "Processing doc: 8800\n",
      "Processing doc: 8900\n",
      "Processing doc: 9000\n",
      "Processing doc: 9100\n",
      "Processing doc: 9200\n",
      "Processing doc: 9300\n",
      "Processing doc: 9400\n",
      "Processing doc: 9500\n",
      "Processing doc: 9600\n",
      "Processing doc: 9700\n",
      "Processing doc: 9800\n",
      "Processing doc: 9900\n",
      "Processing doc: 10000\n"
     ]
    }
   ],
   "source": [
    "# Compute embeddings for the first 1000 documents\n",
    "count = 1\n",
    "for document in preprocessed_documents[:10000]:\n",
    "    if count % 100 == 0:\n",
    "        print(\"Processing doc:\",count)\n",
    "    # Ensure the document is a string\n",
    "    document = str(document)\n",
    "    # print(\"Processing sentence embeddings:\",count)\n",
    "    minilm_embeddings.append(get_document_embedding(document))\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>NearestNeighbors(algorithm=&#x27;brute&#x27;, metric=&#x27;cosine&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;NearestNeighbors<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.neighbors.NearestNeighbors.html\">?<span>Documentation for NearestNeighbors</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>NearestNeighbors(algorithm=&#x27;brute&#x27;, metric=&#x27;cosine&#x27;)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "NearestNeighbors(algorithm='brute', metric='cosine')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocessed_documents = preprocess_documents(anthology_sample)\n",
    "# minilm_embeddings2 = [get_mpnet_embedding(doc) for doc in preprocessed_documents[:10]]  # Only first 1000 for k-NN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "# knn2 = NearestNeighbors(n_neighbors=5, metric='cosine').fit(minilm_embeddings)\n",
    "\n",
    "# Fit NearestNeighbors model\n",
    "nn_minilm = NearestNeighbors(n_neighbors=5, metric='cosine', algorithm='brute')\n",
    "nn_minilm.fit(minilm_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices of nearest neighbors: [[ 31  85 181 149  87]]\n",
      "Distances to nearest neighbors: [[0.74960506 0.7569393  0.76180005 0.7783924  0.78525746]]\n"
     ]
    }
   ],
   "source": [
    "# Get embedding for the query sentence\n",
    "test_query = queries[\"queries\"][18]['q']\n",
    "test_query_sen_embed = get_document_embedding(test_query).reshape(1, -1)\n",
    "# get_nearest_neighbors(test_query_sen_embed,nn_minilm,1)\n",
    "# get_document_embedding\n",
    "\n",
    "distances, indices = nn_minilm.kneighbors(test_query_sen_embed)\n",
    "\n",
    "# Print the indices and distances of the nearest neighbors\n",
    "print(\"Indices of nearest neighbors:\", indices)\n",
    "print(\"Distances to nearest neighbors:\", distances)\n",
    "\n",
    "# Print the nearest neighbor sentences\n",
    "nearest_neighbors = [preprocessed_documents[idx] for idx in indices[0]]\n",
    "# print(\"Nearest neighbor sentences:\", nearest_neighbors)\n",
    "# query_embedding2 = get_mpnet_embedding(test_query).reshape(1, -1)\n",
    "# distances, indices = knn2.kneighbors(query_embedding2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BELOW Cell is VERY IMPORTANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth question: What is the name of the research initiative creating resources for African languages?\n",
      "Ground truth answer: Masakhane. (Masakha is also fine.)\n",
      "Ground truth references: ['2023.acl-long.796', '2023.acl-long.609', '2023.ijcnlp-main.10']\n",
      "OUR references [['2020.lrec-1.463', '2023.sigtyp-1.17', 'D17-1005', 'O16-1024', 'D18-1273']]\n"
     ]
    }
   ],
   "source": [
    "### Above results tell us following:\n",
    "print(\"Ground truth question:\", queries[\"queries\"][50]['q'])\n",
    "print(\"Ground truth answer:\", queries[\"queries\"][50]['a'])\n",
    "print(\"Ground truth references:\", queries[\"queries\"][50]['r'])\n",
    "print(\"OUR references\", [anthology_sample[idx][\"acl_id\"] for idx in indices])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "## Retrieving the query\n",
    "# Function to retrieve the ground truth for a given query\n",
    "def get_ground_truth(query):\n",
    "    for q in queries[\"queries\"]:\n",
    "        if q[\"q\"] == query:\n",
    "            # return q[\"r\"]\n",
    "            return [r for r in q[\"r\"]]\n",
    "    return None\n",
    "# ground_truth = get_ground_truth(query)\n",
    "\n",
    "# Function to compare the result with the ground truth for a single query\n",
    "# Function to calculate average precision for a single query\n",
    "def average_precision(retrieved_docs, ground_truth_ids):\n",
    "    if not ground_truth_ids:\n",
    "        return 0\n",
    "    retrieved_docs_set = set(retrieved_docs)\n",
    "    ground_truth_set = set(ground_truth_ids)\n",
    "    \n",
    "    num_relevant = 0\n",
    "    precision_sum = 0\n",
    "    for i, doc in enumerate(retrieved_docs):\n",
    "        if doc in ground_truth_set:\n",
    "            num_relevant += 1\n",
    "            precision_sum += num_relevant / (i + 1)\n",
    "    \n",
    "    return precision_sum / len(ground_truth_set)\n",
    "\n",
    "# Function to compare the result with the ground truth for a single query\n",
    "def compare_with_ground_truth(query_text, nn_model, dataset, ground_truth_function, k):\n",
    "    # print(\"cgt k=\",k)\n",
    "    # Get nearest neighbors\n",
    "    indices = get_nearest_neighbors(query_text, nn_model, k)\n",
    "    \n",
    "    # Convert numpy.int64 to Python int\n",
    "    indices = [int(i) for i in indices]\n",
    "    \n",
    "    # Retrieve document IDs for nearest neighbors\n",
    "    retrieved_docs = [dataset[i]['acl_id'] for i in indices]\n",
    "    \n",
    "    # Get ground truth\n",
    "    ground_truth_ids = ground_truth_function(query_text)\n",
    "    \n",
    "    if not ground_truth_ids:\n",
    "        return 0, 0, 0, 0, 0, 0, 0  # Return zeros if no ground truth is available\n",
    "    \n",
    "    # Calculate true positives, false positives, and false negatives\n",
    "    tp = len(set(retrieved_docs) & set(ground_truth_ids))\n",
    "    fp = len(retrieved_docs) - tp\n",
    "    fn = len(ground_truth_ids) - tp\n",
    "    \n",
    "    # Calculate precision, recall, and F1-score\n",
    "    precision = tp / len(retrieved_docs) if retrieved_docs else 0\n",
    "    recall = tp / len(ground_truth_ids) if ground_truth_ids else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) else 0\n",
    "    \n",
    "    # Calculate average precision\n",
    "    ap = average_precision(retrieved_docs, ground_truth_ids)\n",
    "    \n",
    "    return precision, recall, f1, ap, tp, fp, fn\n",
    "\n",
    "# Function to evaluate the model on all queries\n",
    "def evaluate_model_on_all_queries(queries, embeddings, dataset, ground_truth_function, k):\n",
    "    # Fit NearestNeighbors model\n",
    "    print(\"k=\",k)\n",
    "    nn = NearestNeighbors(n_neighbors=k, metric='cosine', algorithm='brute')\n",
    "    nn.fit(embeddings)\n",
    "\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "    ap_scores = []\n",
    "    total_tp = total_fp = total_fn = 0\n",
    "\n",
    "    for query in queries[\"queries\"]:\n",
    "        query_text = query[\"q\"]\n",
    "        precision, recall, f1, ap, tp, fp, fn = compare_with_ground_truth(query_text, nn, dataset, ground_truth_function, k)\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "        ap_scores.append(ap)\n",
    "        total_tp += tp\n",
    "        total_fp += fp\n",
    "        total_fn += fn\n",
    "    \n",
    "    # Calculate macro average metrics\n",
    "    macro_precision = np.mean(precision_scores)\n",
    "    macro_recall = np.mean(recall_scores)\n",
    "    macro_f1 = np.mean(f1_scores)\n",
    "    \n",
    "    # Calculate micro average metrics\n",
    "    micro_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) else 0\n",
    "    micro_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) else 0\n",
    "    micro_f1 = 2 * (micro_precision * micro_recall) / (micro_precision + micro_recall) if (micro_precision + micro_recall) else 0\n",
    "    \n",
    "    # Calculate mean AP\n",
    "    mean_ap = np.mean(ap_scores)\n",
    "    \n",
    "    return macro_precision, macro_recall, macro_f1, micro_precision, micro_recall, micro_f1, mean_ap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nearest_neighbors(query_text, nn_model, k):\n",
    "    # query_embedding = get_aggregated_word_embeddings([query_text])\n",
    "    # print(query_text)\n",
    "    # test_query_word_embed = get_aggregated_word_embeddings(query_text).reshape(1, -1)\n",
    "    test_query_word_embed = get_document_embedding(query_text).reshape(1, -1)\n",
    "    distances, indices = nn_model.kneighbors(test_query_word_embed, n_neighbors=k)\n",
    "    return indices[0]\n",
    "\n",
    "def nearest_neighbour(embeddings): # can be tfidf_matrix or LSI_matrix doesnt\n",
    "    nn = NearestNeighbors(n_neighbors=1, metric='cosine',algorithm = 'brute')\n",
    "    nn.fit(embeddings)\n",
    "    \n",
    "# test_query = queries[\"queries\"][0]['q']\n",
    "# get_nearest_neighbors(test_query,nn_minilm,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESULTS when summarizing preprocessing text using ntlk library and minilm_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2') for sentence embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 5\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Invalid key: 2471 is out of bounds for size 1153",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m----> 2\u001b[0m macro_precision, macro_recall, macro_f1, micro_precision, micro_recall, micro_f1, mean_ap \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model_on_all_queries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminilm_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manthology_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_ground_truth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMacro Average Precision: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmacro_precision\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMacro Average Recall: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmacro_recall\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[69], line 78\u001b[0m, in \u001b[0;36mevaluate_model_on_all_queries\u001b[1;34m(queries, embeddings, dataset, ground_truth_function, k)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m query \u001b[38;5;129;01min\u001b[39;00m queries[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqueries\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m     77\u001b[0m     query_text \u001b[38;5;241m=\u001b[39m query[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 78\u001b[0m     precision, recall, f1, ap, tp, fp, fn \u001b[38;5;241m=\u001b[39m \u001b[43mcompare_with_ground_truth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mground_truth_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m     precision_scores\u001b[38;5;241m.\u001b[39mappend(precision)\n\u001b[0;32m     80\u001b[0m     recall_scores\u001b[38;5;241m.\u001b[39mappend(recall)\n",
      "Cell \u001b[1;32mIn[69], line 40\u001b[0m, in \u001b[0;36mcompare_with_ground_truth\u001b[1;34m(query_text, nn_model, dataset, ground_truth_function, k)\u001b[0m\n\u001b[0;32m     37\u001b[0m indices \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m indices]\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Retrieve document IDs for nearest neighbors\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m retrieved_docs \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43macl_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Get ground truth\u001b[39;00m\n\u001b[0;32m     43\u001b[0m ground_truth_ids \u001b[38;5;241m=\u001b[39m ground_truth_function(query_text)\n",
      "Cell \u001b[1;32mIn[69], line 40\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     37\u001b[0m indices \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m indices]\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Retrieve document IDs for nearest neighbors\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m retrieved_docs \u001b[38;5;241m=\u001b[39m [\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macl_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m indices]\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Get ground truth\u001b[39;00m\n\u001b[0;32m     43\u001b[0m ground_truth_ids \u001b[38;5;241m=\u001b[39m ground_truth_function(query_text)\n",
      "File \u001b[1;32mc:\\Users\\mjova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\arrow_dataset.py:2861\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2859\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[0;32m   2860\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2861\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mjova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\arrow_dataset.py:2845\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[1;34m(self, key, **kwargs)\u001b[0m\n\u001b[0;32m   2843\u001b[0m format_kwargs \u001b[38;5;241m=\u001b[39m format_kwargs \u001b[38;5;28;01mif\u001b[39;00m format_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m   2844\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[1;32m-> 2845\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m \u001b[43mquery_table\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2846\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m format_table(\n\u001b[0;32m   2847\u001b[0m     pa_subtable, key, formatter\u001b[38;5;241m=\u001b[39mformatter, format_columns\u001b[38;5;241m=\u001b[39mformat_columns, output_all_columns\u001b[38;5;241m=\u001b[39moutput_all_columns\n\u001b[0;32m   2848\u001b[0m )\n\u001b[0;32m   2849\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[1;32mc:\\Users\\mjova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\formatting\\formatting.py:587\u001b[0m, in \u001b[0;36mquery_table\u001b[1;34m(table, key, indices)\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    586\u001b[0m     size \u001b[38;5;241m=\u001b[39m indices\u001b[38;5;241m.\u001b[39mnum_rows \u001b[38;5;28;01mif\u001b[39;00m indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m table\u001b[38;5;241m.\u001b[39mnum_rows\n\u001b[1;32m--> 587\u001b[0m     \u001b[43m_check_valid_index_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[38;5;66;03m# Query the main table\u001b[39;00m\n\u001b[0;32m    589\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\mjova\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\formatting\\formatting.py:527\u001b[0m, in \u001b[0;36m_check_valid_index_key\u001b[1;34m(key, size)\u001b[0m\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m    526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (key \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m+\u001b[39m size \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (key \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m size):\n\u001b[1;32m--> 527\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid key: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is out of bounds for size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    528\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    529\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mslice\u001b[39m):\n",
      "\u001b[1;31mIndexError\u001b[0m: Invalid key: 2471 is out of bounds for size 1153"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "macro_precision, macro_recall, macro_f1, micro_precision, micro_recall, micro_f1, mean_ap = evaluate_model_on_all_queries(queries, minilm_embeddings, anthology_sample, get_ground_truth, k)\n",
    "print(f\"Macro Average Precision: {macro_precision:.4f}\")\n",
    "print(f\"Macro Average Recall: {macro_recall:.4f}\")\n",
    "print(f\"Macro Average F1-Score: {macro_f1:.4f}\")\n",
    "print(f\"Micro Average Precision: {micro_precision:.4f}\")\n",
    "print(f\"Micro Average Recall: {micro_recall:.4f}\")\n",
    "print(f\"Micro Average F1-Score: {micro_f1:.4f}\")\n",
    "print(f\"Mean Average Precision (mAP): {mean_ap:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install chromadb openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "\n",
    "CHROMA_DATA_PATH = \"chroma_data/\"\n",
    "EMBED_MODEL = \"all-MiniLM-L6-v2\"\n",
    "COLLECTION_NAME = \"demo_docs\"\n",
    "client = chromadb.PersistentClient(path=CHROMA_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "...     model_name=EMBED_MODEL\n",
    "... )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_texts = [f'{d[\"full_text\"]}' for d in anthology_sample]\n",
    "abstracts = [f'{d[\"abstract\"]}' for d in anthology_sample]\n",
    "ids=[f\"{i}\" for i in range(len(anthology_sample))]\n",
    "acl_ids = [f'acl_id:{d[\"acl_id\"]}' for d in anthology_sample]\n",
    "authors=[{\"author\":{d[\"author\"]}} for d in anthology_sample]\n",
    "# collection.add(documents=preprocessed_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.delete_collection(COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = client.create_collection(\n",
    "...     name=COLLECTION_NAME,\n",
    "...     embedding_function=embedding_func,\n",
    "...     metadata={\"hnsw:space\": \"cosine\"},\n",
    "... )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.add(\n",
    "...     documents=preprocessed_documents,\n",
    "...     ids=ids,\n",
    "# ...     metadatas=authors\n",
    "... )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_results = collection.query(\n",
    "...     query_texts=[\"What is the name of the research initiative creating resources for African languages?\"],\n",
    "...     n_results=5,\n",
    "... )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# These are the search results on example question (queries[\"queries\"][50]) using ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L14-1106\n",
      "2023.sigtyp-1.17\n",
      "L16-1719\n",
      "2021.mrl-1.11\n",
      "W14-2212\n"
     ]
    }
   ],
   "source": [
    "# query_results[\"ids\"][0]\n",
    "\n",
    "for id in query_results[\"ids\"][0]:\n",
    "    print(anthology_sample[int(id)][\"acl_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here are our results for the same question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUR references [['2020.lrec-1.463', '2023.sigtyp-1.17', 'D17-1005', 'O16-1024', 'D18-1273']]\n"
     ]
    }
   ],
   "source": [
    "print(\"OUR references\", [anthology_sample[idx][\"acl_id\"] for idx in indices])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ground truth for the same query (q50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth question: What is the name of the research initiative creating resources for African languages?\n",
      "Ground truth answer: Masakhane. (Masakha is also fine.)\n",
      "Ground truth references: ['2023.acl-long.796', '2023.acl-long.609', '2023.ijcnlp-main.10']\n"
     ]
    }
   ],
   "source": [
    "print(\"Ground truth question:\", queries[\"queries\"][50]['q'])\n",
    "print(\"Ground truth answer:\", queries[\"queries\"][50]['a'])\n",
    "print(\"Ground truth references:\", queries[\"queries\"][50]['r'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion: It seems that our sentence transformer provides closer matches to the ground truth than results from ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = queries[\"queries\"][50]\n",
    "user_prompt = queries[\"queries\"][50][\"q\"]\n",
    "user_prompt= \"Which subword tokenizers can be trained with semi-supervised data?\"\n",
    "user_prompt = queries[\"queries\"][97][\"q\"]\n",
    "\n",
    "# for hit in hits:\n",
    "#   print(hit.payload, \"score:\", hit.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.7133451 , 0.7148911 , 0.7373704 , 0.73797905, 0.7390915 ]],\n",
       "       dtype=float32),\n",
       " array([[ 38, 170, 121, 166,  74]], dtype=int64))"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances, indices = nn_minilm.kneighbors(get_document_embedding(user_prompt).reshape(1, -1))\n",
    "distances, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = []\n",
    "for i,cosine in enumerate(distances):\n",
    "    acl_anthology_index = sentence_to_doc_map[indices[0][i]]\n",
    "    # matches.append(anthology_sample[int(indices[0][i])])\n",
    "    match_d = anthology_sample[int(indices[0][i])]\n",
    "    # match_d[\"distance\"] = cosine[i]\n",
    "    match_d[\"rank\"] = f'{cosine[i]}'\n",
    "    # matches.append(anthology_sample[acl_anthology_index])\n",
    "    matches.append(match_d)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ontology-based Contextual Coherence Scoring'"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches[0][\"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acl_id': 'W03-2115',\n",
       " 'abstract': 'In this paper we present a contextual extension to ONTOSCORE, a system for scoring sets of concepts on the basis of an ontology. We apply the contextually enhanced system to the task of scoring alternative speech recognition hypotheses (SRH) in terms of their semantic coherence. We conducted several annotation experiments and showed that human annotators can reliably differentiate between semantically coherent and incoherent speech recognition hypotheses (both with and without discourse context). We also showed, that annotators can reliably identify the overall best hypothesis from a given n-best list. While the original ONTOSCORE system correctly assigns the highest score to 84.06% of the corpus, the inclusion of the conceptual context increases the number of correct classifications to yield 86.76%, given a baseline of 63.91% in both cases.',\n",
       " 'full_text': \"In this paper we present a contextual extension to ONTOSCORE, a system for scoring sets of concepts on the basis of an ontology. We apply the contextually enhanced system to the task of scoring alternative speech recognition hypotheses (SRH) in terms of their semantic coherence. We conducted several annotation experiments and showed that human annotators can reliably differentiate between semantically coherent and incoherent speech recognition hypotheses (both with and without discourse context). We also showed, that annotators can reliably identify the overall best hypothesis from a given n-best list. While the original ONTOSCORE system correctly assigns the highest score to 84.06% of the corpus, the inclusion of the conceptual context increases the number of correct classifications to yield 86.76%, given a baseline of 63.91% in both cases. Introduction Following Allen et al. (2001) , we can distinguish between controlled and conversational dialogue systems. Since controlled and restricted interactions between the user and the system increase recognition and understanding accuracy, such systems are reliable enough to be deployed in various real world applications, e.g. public transportation or cinema information systems. The more conversational a dialogue system becomes, the less predictable are the users' utterances. Recognition and processing become increasingly difficult and unreliable. Today's dialogue systems employ domain-and discourse-specific knowledge bases, so-called ontologies, to represent the individual discourse entities as concepts as well as their relations to each other. In this paper we employ an algorithm for measuring the semantic coherence of sets of concepts using such an ontology and show how its performance can be improved by means of an inclusion of the conceptual context. Thereby creating a method for scoring the contextual coherence of individual sets of concepts. In the following, we will show how the contextual coherence measurement can be applied to estimate how well a given speech recognition hypothesis (SRH) fits with respect to the existing knowledge representation and the given conceptual context, thereby providing a mechanism that increases the robustness and reliability of dialogue systems. We can, therefore, show how the algorithm can be successfully employed by a spoken dialogue system to enhance the interface between automatic speech recognition (ASR) and natural language understanding (NLU). In Section 2 we discuss the problem of scoring and classifying SRHs in terms of their semantic coherence followed by a description of our annotation experiments and the corresponding results in Section 3. Section 4 contains a description of the kind of knowledge representations and the algorithm employed by ONTOSCORE. In Section 5 we present the contextually enhanced system. Evaluations of the corresponding system for scoring SRHs are given in Section 6. A conclusion and additional applications are given in Section 7. Semantic Coherence and Speech Recognition Hypotheses While a simple one-best hypothesis interface between ASR and NLU suffices for restricted dialogue systems, more complex systems either operate on nbest lists as ASR output or convert ASR word graphs (Oerder and Ney, 1993) into n-best lists, given the distribution of acoustic and language model scores (Schwartz and Chow, 1990; Tran et al., 1996) . For example, in our data a user expressed the wish to get from Cologne to Heidelberg and then to continue his visit in Heidelberg, as: Facing multiple representations of a single utterance consequently poses the question, which of the different hypotheses corresponds most likely to the user's utterance. Several ways of solving this problem have been proposed and implemented in various systems. Frequently the scores provided by the ASR system itself are used, e.g. acoustic and language model probabilities. More recently also scores provided by the NLU system have been employed, e.g. parsing scores (Engel, 2002) or discourse model scores (Pfleger et al., 2002) . However, these methods often assign very high scores to SRHs which are semantically incoherent and low scores to semantically coherent ones. In the case of Example (1) all scores, i.e. the acoustic, language model, parsing and the ON-TOSCORE scores assign the highest score to Example (1a) (see Table 2 for the actual numbers). SRH 1a can consequently be chosen as the best SRH. As we will show in Section 6, the scoring of the SRHs from Example (2) differs substantially, and only the contextual coherence score manages to pick an adequate SRH. The fact that neither of the other scoring approaches systematically employs the system's knowledge of the domains at hand, can result in passing suboptimal SRHs through the system. This means that, while there was a better representation of the actual utterance in the n-best list, the NLU system is processing an inferior one, thereby causing overall dialogue metrics, in the sense of Walker et al. (2000) , to decrease. We, therefore, propose an alternative way to rank SRHs on the basis of their contextual coherence, i.e. with respect to a given ontology representing the domains of the system and the given conceptual context. Annotation Experiments The experiments reported here are based on the data collected in hidden-operator tests where subjects were prompted to say certain inputs. We obtained 232 dialogues, which were divided into 1479 audio files with single user utterances. Each utterance corresponded to a single intention, e.g. a route-or a sight information request. Firstly, all utterances were also transcribed. Then the audio files were sent to the speech recognizer. We logged the speech recognition output, i.e. n-best lists of SRHs for all utterances. A subset of the corpus was used to log also the scores of the recognizer, parser and that of OntoScore -including context-independent and context-dependent semantic coherence scores. This trial resulted in a sub-corpus of 552 utterances corresponding to 1,375 SRHs along with the respective confidence scores. We, then, conducted several annotation experiments with a two-fold motivation. In the first place, it was necessary to produce a hand-annotated corpus to be used as a gold standard for the evaluation of the contextual coherence scores. Furthermore, we wanted to test whether human subjects were able to annotate the data reliably according to our annotation schemata. We had two annotators specially trained for each of these particular annotation tasks. In an earlier annotation experiment reported in Gurevych et al. (2002) , the task of annotators was to classify a subset of the corpus of SRHs as either coherent or incoherent. Here we randomly mixed SRHs in order to avoid contextual priming. 2 In the first new experiment, a sub-corpus of 552 utterances was annotated within the discourse context, i.e. the SRHs were presented in their original dialogue order. For each SRH, a decision again had to be made whether it is semantically coherent or incoherent with respect to the best SRH representing the previous user utterance. Given a total of 1,375 markables, the annotators reached an agreement of 79.71%, i.e. 1,096 markables. In the second new annotation experiment, the annotators saw the SRHs together with the transcribed user utterances. The task of annotators was to determine the best SRH from the n-best list of SRHs corresponding to a single user utterance. The decision had to be made on the basis of several criteria. The most important criteria was how well the SRH captures the intentional content of the user's utterance. If none of the SRHs captured the user's intention adequately, the decision had to be made by looking at the actual word error rate. In this experiment the inter-annotator agreement was 90.69%, i.e. 1,247 markables out of 1,375. 3 Each corpus was then tranformed into an evaluation gold standard by means of the annotators agreeing on a single solution for the cases of disagreement. The aim of the work presented here, then, was to provide a knowledge-based score, that can be employed by any NLU system to select the best hypothesis from a given n-best list. The corresponding ON-TOSCORE system will be described below, followed by its evaluation against the human gold standards. The Knowledge Base and OntoScore In this section, we provide a description of the underlying algorithm and knowledge sources employed by the original ONTOSCORE system (in press). It is important to note that the ontology employed in this and the previous evaluations existed already and was crafted as a general knowledge representation for various processing modules within the system. 4 Ontologies have traditionally been used to represent general and domain specific knowledge and are employed for various natural language understanding tasks, e.g. semantic interpretation (Allen, 1987) and in spoken dialogue systems, e.g. for discourse modeling, modality fusion and dialogue management, see also Porzel et al. (2003) for an overview. ONTOSCORE offers an additional way of employing ontologies, i.e. to use the knowledge modeled therein as the basis for evaluating the semantic coherence of sets of concepts. It can be employed independently of the specific ontology language used, as the underlying algorithm operates only on the nodes and named edges of the directed graph represented by the ontology. The specific knowledge base, e.g. written in DAML+OIL or OWL, 5 is converted into a graph, consisting of the class hierarchy, with each class corresponding to a concept representing either an entity or a process and their slots, i.e. the named edges of the graph corresponding to the class properties, constraints and restrictions. The ontology employed for the evaluation has about 730 concepts and 200 relations. It includes a generic top-level ontology whose purpose is to provide a basic structure of the world, i.e. abstract classes to divide the universe in distinct parts as resulting from the ontological analysis. 6 The modeling of Processes and Physical Objects as a kind of event that is continuous and homogeneous in nature, follows the frame semantic analysis used for generating the FRAMENET data (Baker et al., 1998) . The hierarchy of Processes is connected to the hierarchy of Physical Objects via slot-constraint definitions. See also (Gurevych et al., 2003b) for a further description of the ontology. ONTOSCORE performs a number of processing steps. A first preprocessing step is to convert each SRH into a concept representation (CR). For that purpose we augmented system's lexicon with specific concept mappings. That is, for each entry in the lexicon either zero, one or many corresponding concepts where added. A simple vector of conceptscorresponding to the words in the SRH for which entries in the lexicon exist -constitutes each resulting CR. All other words with empty concept mappings, e.g. articles and aspectual markers, are ignored in the conversion. Due to lexical ambiguity, i.e. the one to many word -concept mappings, this processing step yields a set I = {CR 1 , CR 2 , . . . , CR n } of possible interpretations for each SRH. ONTOSCORE converts the domain model, i.e. an ontology, into a directed graph with concepts as nodes and relations as edges. In order to find the shortest path between two concepts, ONTOSCORE employs the single source shortest path algorithm of Dijkstra (Cormen et al., 1990) . Thus, the minimal paths connecting a given concept c i with every other concept in CR (excluding c i itself) are selected, resulting in an n × n matrix of the respective paths. To score the minimal paths connecting all concepts with each other in a given CR, we adopted a method proposed by Demetriou and Atwell (1994) to score the semantic coherence of alternative sentence interpretations against graphs based on the Longman Dictionary of Contemporary English (LDOCE). As defined by Demetriou and Atwell (1994) , R = {r 1 , r 2 , . . . , r n } is the set of direct relations (both isa and semantic relations) that can connect two nodes (concepts); and W = {w 1 , w 2 , . . . , w n } is the set of corresponding weights, where the weight of each isa relation is set to 0 and that of each other relation to 1. The algorithm selects from the set of all paths between two concepts the one with the smallest weight, i.e. the cheapest. The distances between all concept pairs in CR are summed up to a total score. The set of concepts with the lowest aggregate score represents the combination with the highest semantic relatedness. The ensuing distance between two concepts, e.g. D(c i , c j ) is, then, defined as the minimum score derived between c i and c j . Demetriou and Atwell (1994) do not provide concrete evaluation results for the method. Also, their algorithm only allows for a relative judgment stating which of a set of interpretations given a single sentence is more semantically related. Since our objective is to compute coherence scores of arbitrary CRs on an absolute scale, certain extensions were necessary. In this application the CRs to be scored can differ in terms of their content, the number of concepts contained therein and their mappings to the original SRH. Moreover, in order to achieve absolute values, the final score should be related to the number of concepts in an individual set and the number of words in the original SRH. Therefore, the results must be normalized in order to allow for evaluation, comparability and clearer interpretation of the semantic coherence scores. We modified the algorithm described above to make it applicable and evaluatable with respect to the task at hand as well as other possible tasks. The basic idea is to calculate a score based on the path distances in CR. Since short distances indicate coherence and many concept pairs in a given CR may have no connecting path, we define the distance between two concepts c i and c j that are not connected in the knowledge base as D max . This maximum value can also serve as a maximum for long distances and can thus help to prune the search tree for long paths. This constant has to be set according to the structure of the knowledge base. For example, employing the ontology described above, the maximum distance between two concepts does not exceed ten and we chose in that case D max = 10. We can now define the semantic coherence score for CR as the average path length between all concept pairs in S(CR) = c i ,c j ∈CR,c i =c j D(c i , c j ) |CR| 2 − |CR| Since the ontology is a directed graph, we have |CR| 2 − |CR| pairs of concepts with possible directed connections, i.e., a path from concept c i to concept c j may be completely different to that from c j to c i or even be missing. As a symmetric alternative, we may want to consider a path from c i to c j and a path from c j to c i to be semantically equivalent and thus model every relation in a bidirectional way. We can then compute a symmetric score S (CR) as S (CR) = 2 c i ,c j ∈CR,i<j min(D(c i , c j )D(c j , c i )) |CR| 2 − |CR| ONTOSCORE implements both options. As the ontology currently employed features mostly unidirectional relations we chose the S (CR) function for the evaluation, i.e. only the best path D(c i , c j ) between a given pair of concepts, regardless of the direction, is taken into account. A detailed description of the original system can be found in (Gurevych et al., 2003a) . Contextual Coherence Scoring The contextually enhanced ONTOSCORE system performs a number of additional processing steps, each of them will be described below. Scoring Conceptual Context Representations A necessary preprocessing step for the conceptual context scoring of SRHs is to build a conceptual context representation CR (SRH n+1 ) resulting from a pair of concept representations: -a concept representation of the SRH to be scored, i.e. CR(SRH n+1 ), -and a concept representation of the preceding utterance's SRH, i.e. CR(SRH n ). For that purpose, the ONTOSCORE stores the best concept representation from each dialogue turn as CR best (SRH). By the best CR we mean the interpretation which received the highest score from the ONTOSCORE system, from the list of alternative interpretations of the utterance. For example CR best for the utterance shown in Example ( 1 ) is the CR of the SRH given in (1e), i.e. {EmotionExperiencerSubjectProcess, Person, Two-PointRelation, Route, Town, Town}. To produce a conceptual context representation for SRH n+1 , we build a union of each of its possible interpretations I = {CR 1 , CR 2 , . . . , CR n } with the stored CR best (SRH n ) from the previous utterance. This results in a contextually augmented new set I = {CR 1 , CR 2 , . . . , CR n } representing possible conceptual context interpretations of SRH n+1 as shown in Table 1 . In order to score the alternative conceptual context representations defined by I (SRH n+1 ), the formula for S (CR) is employed. This means that we calculate a conceptual context coherence score S for each conceptual context representation CR . We also perform an inverse linear transformation of the scores resulting in numbers from 0 to 1, so that higher scores indicate better contextual coherence. I(SRH n+1 ) I (SRH n+1 ) CR 1 ∪ CR best (SRH n ) = CR 1 CR 2 ∪ CR best (SRH n ) = CR 2 ... ... ... CR n ∪ CR best (SRH n ) = CR n ONTOSCORE at Work Looking at an example of ONTOSCORE at work, we will examine the following discourse fragment consisting of the two sequential utterances given in Example (1) and (2). As shown in As evident from Table 3 , CR best corresponds to Example 2e. This means that 2e constitutes a more contextually coherent concept structure than the alternative SRHs. This SRH was also labeled both as the best and as a coherent SRH by the annotators. Evaluation The ONTOSCORE software runs as a module in the SMARTKOM multi-modal and multi-domain spoken dialogue system (Wahlster et al., 2001) . The system features the combination of speech and gesture as its input and output modalities. The domains of the system include cinema and TV program information, home electronic device control as well as mobile services for tourists, e.g. tour planning and sights information. ONTOSCORE operates on n-best lists of SRHs produced by the language interpretation module out of the ASR word graphs. It computes a numerical ranking of alternative SRHs and thus provides an important aid to the spoken language understanding component. More precisely, the task of ON-TOSCORE in the system is to identify the best SRH suitable for further processing and evaluate it in terms of its contextual coherence against the domain and discourse knowledge. The ONTOSCORE module currently employs two knowledge sources, an ontology (about 730 concepts and 200 relations) and a lexicon (ca. 3.600 words) with word to concept mappings, covering the respective domains of the system. The evaluation of ONTOSCORE was carried out on a set of 95 dialogues. The resulting dataset contained 552 utterances resulting in 1,375 SRHs, corresponding to an average of 2.49 SRHs per user utterance. The corpus had been annotated by humans subjects according to two separate annotation schemata. The results of annotation experiments are reported in Section 3. Identifying the Best SRH The task of ONTOSCORE in our multimodal dialogue system is to determine the best SRH from the n-best list of SRHs corresponding to a given user utterance. The baseline for this evaluation was computed by adding the individual ratios of utterance/SRHs -corresponding to the likelihood of guessing the best one in each individual case -and dividing it by the number of utterances -yielding the overall likelihood of guessing the best one 63.91%. The accuracy of ONTOSCORE on this task amounts to 86.76%. This means that in 86.76% of all cases the best SRH defined by the human gold standard is among the best scored by the ON-TOSCORE module. The ONTOSCORE module without the conceptual context feature yields the accuracy of only 84.06% on the same task. This suggests that the overall results in identifying the best SRH in the speech recognizer output can by improved by taking the knowledge of conceptual context into account. Classifying the SRHs as Semantically Coherent versus Incoherent For this evaluation we used the same corpus, where each SRH was labeled as being either semantically coherent versus incoherent with respect to the previous discourse context. We defined a baseline based on the majority class, i.e. coherent, in the corpus, 63.05%. In order to obtain a binary classification into semantically coherent and incoherent SRHs, a cutoff threshold must be set. Employing a cutoff threshold of 0.44, we find that the contextually enhanced ONTOSCORE system correctly classifies 70.98% of SRHs in the corpus. This indicates the improvement of 7.93% over the baseline. We also conducted the same classification experiment with ONTOSCORE without using the conceptual context feature. In this case we obtained 69.96% accuracy. From these results we can conclude that the task of an absolute classification of coherent versus incoherent is substantially more difficult than that of determining the best SRH, both for human annotators (see Section 3) and for ONTOSCORE. Both human and the system's reliability is lower in the coherent versus incoherent classification task, which allows to classify zero, one or multiple SRHs from one utterance as coherent or incoherent. In both tasks, however, ONTOSCORE's performance mirrors and approaches human performance. Concluding Remarks The contextually enhanced ONTOSCORE system described herein automatically performs ontologybased scoring of sets of concepts which constitute an adequate and suitable representation of a speech recognition hypothesis and the prior conceptual context. This conceptual context is an analogous conceptual representation of the previous user utterance. To date, the algorithm has been implemented in a software which is employed by a multi-domain spoken dialogue system and applied to the task of scoring n-best lists of SRH, thus producing a score expressing how well a given SRH fits within the domain model and the given discourse. In the evaluation of our system we employed an ontology that was not designed for this task, but already existed as the system's internal knowledge representation. As shown above, the inclusion of the conceptual discourse context yields an improvement of almost 3% as compared to the context-independent system. As future work we will examine how the computation of a contextual coherence score, i.e. how well a given SRH fits within the domain model with respect to the previous discourse, can be em-ployed to detect domain changes in complex multimodal and multi-domain spoken dialogue systems. As one would expect, a contextual coherence score as described above actually decreases when the user changed from one domain to another, which most likely also accounts for a set of the actual misclassifications. As a future enhancement we will integrate and evaluate an automatic domain change detection function, which, if activated, will cause the system to employ the context-independent scoring function. Currently, we are also investigating whether the proposed method can be applied to scoring sets of potential candidates for resolving the semantic interpretation of ambiguous, polysemous and metonymic language use (Porzel and Gurevych, 2003) . Additionally, As ontology building is constly, we examine the feasibility to employ alternative knowledge sources, that are generated automatically from corpora, e.g. via self organizing maps. Acknowledgments There work described was conducted within the SmartKom project partly funded by the German ministry of Research and Technology under grant 01IL95I7 and by the Klaus Tschira Foundation.\",\n",
       " 'year': '2003',\n",
       " 'author': 'Porzel, Robert  and\\nGurevych, Iryna  and\\nM{\\\\\"u}ller, Christof E.',\n",
       " 'title': 'Ontology-based Contextual Coherence Scoring',\n",
       " 'rank': '0.7496050596237183'}"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "openai.api_key  = os.environ['OPENAI_API_KEY']\n",
    "llm_name = \"gpt-3.5-turbo\"\n",
    "\n",
    "# user_p = \"What is the distance from Earth to the Moon?\"\n",
    "user_p = f\"Do you know of any contextual extension to ONTOSCORE? Please only answer which papers are cited in the matching document?\"\n",
    "user_p = f\"Do you know of any contextual extension to ONTOSCORE? Please mention paper acl_id, paper title, and authors of the matching document?\"\n",
    "messages = [\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"You are a document retrieval assistant. When given a document's details, you should provide a summary response including the document's rank, title, and text.\"\n",
    "                        \"If you cannot answer using the sources below, say you don't know. \"\n",
    "        },\n",
    "        {\"role\": \"assistant\", \"content\": str(matches)},\n",
    "        {\"role\": \"user\", \"content\": str({user_p})}\n",
    "]\n",
    "client = openai.OpenAI()\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=messages,\n",
    "  temperature=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-9XgFVpupOgKW58l82DN3AS47ePhrk', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The document with the acl_id \\'W03-2115\\' titled \"Ontology-based Contextual Coherence Scoring\" by Robert Porzel, Iryna Gurevych, and Christof E. Müller presents a contextual extension to ONTOSCORE.', role='assistant', function_call=None, tool_calls=None))], created=1717814257, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=54, prompt_tokens=5403, total_tokens=5457))\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='{\\'I\\'m sorry for any confusion, but I don\\'t have specific information about a contextual extension to ONTOSCORE beyond what was mentioned in the provided paper. The paper you\\'re asking about is \"Ontology-based Contextual Coherence Scoring\" by Porzel, Robert, Gurevych, Iryna, and Müller Christof E., published in 2003. In this paper, they propose a method for scoring the coherence of user utterances with an ontology using context information. However, they don\\'t mention any specific extension to ONTOSCORE in the abstract or the text of the paper.\\'}', role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Now time to connect to the local large language model\n",
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    base_url=\"http://127.0.0.1:8080/v1\", # \"http://<Your api-server IP>:port\"\n",
    "    api_key = \"sk-no-key-required\"\n",
    ")\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"LLaMA_CPP\",\n",
    "    messages=messages\n",
    ")\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-XvoK104dWqPNvaNtdtLxXhbJUrSxR1kt', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='{\\'I\\'m not aware of any specific contextual extension to ONTOSCORE, but the authors of the paper \"Ontology-based Contextual Coherence Scoring\" (Porzel, Robert et al., 2003) suggest that their method could potentially be applied to scoring sets of potential candidates for resolving ambiguous, polysemous and metonymic language use. However, I cannot provide a document rank for this information as it is based on the content of the paper and not on any specific document.\\'}', role='assistant', function_call=None, tool_calls=None))], created=1717810959, model='LLaMA_CPP', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=118, prompt_tokens=391, total_tokens=509))\n"
     ]
    }
   ],
   "source": [
    "print(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
