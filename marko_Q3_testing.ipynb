{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bf4d23e-886f-488e-8a93-80d56bd23876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: numpy in /home/marko/virt/py310/lib/python3.10/site-packages (from faiss-cpu) (1.26.4)\n",
      "Downloading faiss_cpu-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61a8f65d-137c-4b2e-abcd-b95d3f8eb32f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8ab49ecf687401a8a02ddaa1d322cf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c523c19fb84c494c94ad38ba6bed43af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9e0294aadbf434aa67c706abf384f88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b571fb5f4a0b4a0bb6143211de4a3951",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce00dd78e04f42568d7130005d2fab27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "717ba39fff2543ce9e959347b3d19679",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04813ee6556d4719bcb4be01a9e310e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6abb113bdc3a4d03b80ba3c97817d6d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "228605c9a35c41ff82b61a9ac54450c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77bbb225f882403eaf3bcfb42994ee6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2143516397eb43c2af588a0f57330de5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, average_precision_score\n",
    "import time\n",
    "\n",
    "# Load pre-trained model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Example documents with JSON structure\n",
    "documents_json = \"\"\"\n",
    "[\n",
    "    {\"acl_id\": \"1\", \"title\": \"Example title 1\", \"abstract\": \"Example abstract 1\", \"full_text\": \"Sentence 1 of doc 1. Sentence 2 of doc 1.\"},\n",
    "    {\"acl_id\": \"2\", \"title\": \"Example title 2\", \"abstract\": \"Example abstract 2\", \"full_text\": \"Sentence 1 of doc 2. Sentence 2 of doc 2.\"},\n",
    "    {\"acl_id\": \"3\", \"title\": \"Example title 3\", \"abstract\": \"Example abstract 3\", \"full_text\": \"Sentence 1 of doc 3. Sentence 2 of doc 3.\"},\n",
    "    {\"acl_id\": \"4\", \"title\": \"Example title 4\", \"abstract\": \"Example abstract 4\", \"full_text\": \"Sentence 1 of doc 4. Sentence 2 of doc 4.\"}\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "# Load documents from JSON\n",
    "documents = json.loads(documents_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "608fdeb7-bb73-4d21-b81a-414fef522f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth document set\n",
    "ground_truth_json = \"\"\"\n",
    "[\n",
    "    {\"q\": \"Sentence 1 of doc 1\", \"acl_ids\": [\"1\"], \"r\": \"Response 1\"},\n",
    "    {\"q\": \"Sentence 2 of doc 2\", \"acl_ids\": [\"2\"], \"r\": \"Response 2\"},\n",
    "    {\"q\": \"Sentence 1 of doc 3\", \"acl_ids\": [\"3\"], \"r\": \"Response 3\"}\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "ground_truth_docs = json.loads(ground_truth_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "531ad50e-f039-4328-96b3-ae6f91d4279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "941ec4f5-3466-432b-9261-1c7dda06fe9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "111b0ada048c4bd1aaa0b51783fa446b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "27259830"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# R_NUMBER_SEED = 1234567 # Replace this with your own student number\n",
    "R_NUMBER_SEED = 928036 # my student number \n",
    "DOCS_TO_ADD = 1000\n",
    "query_documents = datasets.load_dataset(\"parquet\", data_files=\"./acl_anthology_queries.parquet\")[\"train\"]\n",
    "all_documents = datasets.load_dataset(\"parquet\", data_files=\"./acl_anthology_full.parquet\")[\"train\"]\n",
    "# Shuffle with seed and take only n docs\n",
    "shuffled_documents = all_documents.shuffle(seed=R_NUMBER_SEED)\n",
    "random_documents = shuffled_documents.select(range(DOCS_TO_ADD))\n",
    "# Concatenate relevant documents with random sample and shuffle again\n",
    "anthology_sample = datasets.concatenate_datasets([query_documents, random_documents]).shuffle(seed=R_NUMBER_SEED)\n",
    "# Export to Parquet to avoid downloading full anthology\n",
    "anthology_sample.to_parquet(\"./anthology_sample.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69cc831b-226d-4297-9b4e-878116273081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "queries = json.load(open(\"./acl_anthology_queries.json\", \"r\"))\n",
    "ground_truth_docs = queries[\"queries\"]\n",
    "documents = anthology_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a13949d4-c462-4b9b-9cfa-5fddf8635b92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'q': 'What is the name of the research initiative creating resources for African languages?',\n",
       " 'r': ['2023.acl-long.796', '2023.acl-long.609', '2023.ijcnlp-main.10'],\n",
       " 'a': 'Masakhane. (Masakha is also fine.)'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth_docs[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1fc67137-36df-483c-a959-35207b6307df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc: 1\n",
      "doc: 2\n",
      "doc: 3\n",
      "doc: 4\n",
      "doc: 5\n",
      "doc: 6\n",
      "doc: 7\n",
      "doc: 8\n",
      "doc: 9\n",
      "doc: 11\n",
      "doc: 12\n",
      "doc: 13\n",
      "doc: 14\n",
      "doc: 15\n",
      "doc: 16\n",
      "doc: 17\n",
      "doc: 18\n",
      "doc: 19\n",
      "doc: 21\n",
      "doc: 22\n",
      "doc: 23\n",
      "doc: 24\n",
      "doc: 25\n",
      "doc: 26\n",
      "doc: 27\n",
      "doc: 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7f4eb048d4b0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/marko/virt/py310/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc: 29\n",
      "doc: 31\n",
      "doc: 32\n",
      "doc: 33\n",
      "doc: 34\n",
      "doc: 35\n",
      "doc: 36\n",
      "doc: 37\n",
      "doc: 38\n",
      "doc: 39\n",
      "doc: 41\n",
      "doc: 42\n",
      "doc: 43\n",
      "doc: 44\n",
      "doc: 45\n",
      "doc: 46\n",
      "doc: 47\n",
      "doc: 48\n",
      "doc: 49\n",
      "doc: 51\n",
      "doc: 52\n",
      "doc: 53\n",
      "doc: 54\n",
      "doc: 55\n",
      "doc: 56\n",
      "doc: 57\n",
      "doc: 58\n",
      "doc: 59\n",
      "doc: 61\n",
      "doc: 62\n",
      "doc: 63\n",
      "doc: 64\n",
      "doc: 65\n",
      "doc: 66\n",
      "doc: 67\n",
      "doc: 68\n",
      "doc: 69\n",
      "doc: 71\n",
      "doc: 72\n",
      "doc: 73\n",
      "doc: 74\n",
      "doc: 75\n",
      "doc: 76\n",
      "doc: 77\n",
      "doc: 78\n",
      "doc: 79\n",
      "doc: 81\n",
      "doc: 82\n",
      "doc: 83\n",
      "doc: 84\n",
      "doc: 85\n",
      "doc: 86\n",
      "doc: 87\n",
      "doc: 88\n",
      "doc: 89\n",
      "doc: 91\n",
      "doc: 92\n",
      "doc: 93\n",
      "doc: 94\n",
      "doc: 95\n",
      "doc: 96\n",
      "doc: 97\n",
      "doc: 98\n",
      "doc: 99\n",
      "doc: 101\n",
      "doc: 102\n",
      "doc: 103\n",
      "doc: 104\n",
      "doc: 105\n",
      "doc: 106\n",
      "doc: 107\n",
      "doc: 108\n",
      "doc: 109\n",
      "doc: 111\n",
      "doc: 112\n",
      "doc: 113\n",
      "doc: 114\n",
      "doc: 115\n",
      "doc: 116\n",
      "doc: 117\n",
      "doc: 118\n",
      "doc: 119\n",
      "doc: 121\n",
      "doc: 122\n",
      "doc: 123\n",
      "doc: 124\n",
      "doc: 125\n",
      "doc: 126\n",
      "doc: 127\n",
      "doc: 128\n",
      "doc: 129\n",
      "doc: 131\n",
      "doc: 132\n",
      "doc: 133\n",
      "doc: 134\n",
      "doc: 135\n",
      "doc: 136\n",
      "doc: 137\n",
      "doc: 138\n",
      "doc: 139\n",
      "doc: 141\n",
      "doc: 142\n",
      "doc: 143\n",
      "doc: 144\n",
      "doc: 145\n",
      "doc: 146\n",
      "doc: 147\n",
      "doc: 148\n",
      "doc: 149\n",
      "doc: 151\n",
      "doc: 152\n",
      "doc: 153\n",
      "doc: 154\n",
      "doc: 155\n",
      "doc: 156\n",
      "doc: 157\n",
      "doc: 158\n",
      "doc: 159\n",
      "doc: 161\n",
      "doc: 162\n",
      "doc: 163\n",
      "doc: 164\n",
      "doc: 165\n",
      "doc: 166\n",
      "doc: 167\n",
      "doc: 168\n",
      "doc: 169\n",
      "doc: 171\n",
      "doc: 172\n",
      "doc: 173\n",
      "doc: 174\n",
      "doc: 175\n",
      "doc: 176\n",
      "doc: 177\n",
      "doc: 178\n",
      "doc: 179\n",
      "doc: 181\n",
      "doc: 182\n",
      "doc: 183\n",
      "doc: 184\n",
      "doc: 185\n",
      "doc: 186\n",
      "doc: 187\n",
      "doc: 188\n",
      "doc: 189\n",
      "doc: 191\n",
      "doc: 192\n",
      "doc: 193\n",
      "doc: 194\n",
      "doc: 195\n",
      "doc: 196\n",
      "doc: 197\n",
      "doc: 198\n",
      "doc: 199\n",
      "doc: 201\n",
      "doc: 202\n",
      "doc: 203\n",
      "doc: 204\n",
      "doc: 205\n",
      "doc: 206\n",
      "doc: 207\n",
      "doc: 208\n",
      "doc: 209\n",
      "doc: 211\n",
      "doc: 212\n",
      "doc: 213\n",
      "doc: 214\n",
      "doc: 215\n",
      "doc: 216\n",
      "doc: 217\n",
      "doc: 218\n",
      "doc: 219\n",
      "doc: 221\n",
      "doc: 222\n",
      "doc: 223\n",
      "doc: 224\n",
      "doc: 225\n",
      "doc: 226\n",
      "doc: 227\n",
      "doc: 228\n",
      "doc: 229\n",
      "doc: 231\n",
      "doc: 232\n",
      "doc: 233\n",
      "doc: 234\n",
      "doc: 235\n",
      "doc: 236\n",
      "doc: 237\n",
      "doc: 238\n",
      "doc: 239\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m doc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabstract\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: full_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabstract\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m \n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m doc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull_text\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: full_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m \n\u001b[0;32m---> 13\u001b[0m spacy_doc \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [sent\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m spacy_doc\u001b[38;5;241m.\u001b[39msents]\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences:\n",
      "File \u001b[0;32m~/virt/py310/lib/python3.10/site-packages/spacy/language.py:1049\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     error_handler \u001b[38;5;241m=\u001b[39m proc\u001b[38;5;241m.\u001b[39mget_error_handler()\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1049\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mproc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcomponent_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1051\u001b[0m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE109\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/virt/py310/lib/python3.10/site-packages/spacy/pipeline/trainable_pipe.pyx:52\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/virt/py310/lib/python3.10/site-packages/spacy/pipeline/tok2vec.py:126\u001b[0m, in \u001b[0;36mTok2Vec.predict\u001b[0;34m(self, docs)\u001b[0m\n\u001b[1;32m    124\u001b[0m     width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mget_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnO\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39malloc((\u001b[38;5;241m0\u001b[39m, width)) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs]\n\u001b[0;32m--> 126\u001b[0m tokvecs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokvecs\n",
      "File \u001b[0;32m~/virt/py310/lib/python3.10/site-packages/thinc/model.py:334\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OutT:\n\u001b[1;32m    331\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;124;03m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/virt/py310/lib/python3.10/site-packages/thinc/layers/chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[0;32m~/virt/py310/lib/python3.10/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/virt/py310/lib/python3.10/site-packages/thinc/layers/with_array.py:42\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, Xseq, is_train)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m](Xseq, is_train)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Tuple[SeqT, Callable], \u001b[43m_list_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/virt/py310/lib/python3.10/site-packages/thinc/layers/with_array.py:77\u001b[0m, in \u001b[0;36m_list_forward\u001b[0;34m(model, Xs, is_train)\u001b[0m\n\u001b[1;32m     75\u001b[0m lengths \u001b[38;5;241m=\u001b[39m NUMPY_OPS\u001b[38;5;241m.\u001b[39masarray1i([\u001b[38;5;28mlen\u001b[39m(seq) \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m Xs])\n\u001b[1;32m     76\u001b[0m Xf \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mflatten(Xs, pad\u001b[38;5;241m=\u001b[39mpad)\n\u001b[0;32m---> 77\u001b[0m Yf, get_dXf \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackprop\u001b[39m(dYs: ListXd) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ListXd:\n\u001b[1;32m     80\u001b[0m     dYf \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mflatten(dYs, pad\u001b[38;5;241m=\u001b[39mpad)\n",
      "File \u001b[0;32m~/virt/py310/lib/python3.10/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/virt/py310/lib/python3.10/site-packages/thinc/layers/chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[0;32m~/virt/py310/lib/python3.10/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/virt/py310/lib/python3.10/site-packages/thinc/layers/residual.py:41\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m d_output \u001b[38;5;241m+\u001b[39m dX\n\u001b[0;32m---> 41\u001b[0m Y, backprop_layer \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [X[i] \u001b[38;5;241m+\u001b[39m Y[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X))], backprop\n",
      "File \u001b[0;32m~/virt/py310/lib/python3.10/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/virt/py310/lib/python3.10/site-packages/thinc/layers/chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[0;32m~/virt/py310/lib/python3.10/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/virt/py310/lib/python3.10/site-packages/thinc/layers/chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "    \u001b[0;31m[... skipping similar frames: Model.__call__ at line 310 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/virt/py310/lib/python3.10/site-packages/thinc/layers/chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[0;32m~/virt/py310/lib/python3.10/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/virt/py310/lib/python3.10/site-packages/thinc/layers/maxout.py:52\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     50\u001b[0m W \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_param(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     51\u001b[0m W \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mreshape2f(W, nO \u001b[38;5;241m*\u001b[39m nP, nI)\n\u001b[0;32m---> 52\u001b[0m Y \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgemm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrans2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m Y \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mreshape1f(b, nO \u001b[38;5;241m*\u001b[39m nP)\n\u001b[1;32m     54\u001b[0m Z \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mreshape3f(Y, Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], nO, nP)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Split full_text into sentences and create a mapping to document ids\n",
    "all_sentences = []\n",
    "sentence_to_doc_map = []\n",
    "\n",
    "count = 0\n",
    "for doc in documents:\n",
    "    if count % 10 == 0:\n",
    "        print(\"doc:\",count)\n",
    "    full_text = \"\"\n",
    "    if doc[\"title\"] is not None: full_text = f'{doc[\"title\"]}.' \n",
    "    if doc[\"abstract\"] is not None: full_text = f'{full_text} {doc[\"abstract\"]}.' \n",
    "    if doc[\"full_text\"] is not None: full_text = f'{full_text} {doc[\"full_text\"]}.' \n",
    "    spacy_doc = nlp(full_text)\n",
    "    sentences = [sent.text.strip() for sent in spacy_doc.sents]\n",
    "    for sentence in sentences:\n",
    "        all_sentences.append(sentence)\n",
    "        sentence_to_doc_map.append(doc[\"acl_id\"])\n",
    "    count +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ee311db-5502-4e63-b53c-b43ab27a94ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/marko/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc: 10\n",
      "doc: 20\n",
      "doc: 30\n",
      "doc: 40\n",
      "doc: 50\n",
      "doc: 60\n",
      "doc: 70\n",
      "doc: 80\n",
      "doc: 90\n",
      "doc: 100\n",
      "doc: 110\n",
      "doc: 120\n",
      "doc: 130\n",
      "doc: 140\n",
      "doc: 150\n",
      "doc: 160\n",
      "doc: 170\n",
      "doc: 180\n",
      "doc: 190\n",
      "doc: 200\n",
      "doc: 210\n",
      "doc: 220\n",
      "doc: 230\n",
      "doc: 240\n",
      "doc: 250\n",
      "doc: 260\n",
      "doc: 270\n",
      "doc: 280\n",
      "doc: 290\n",
      "doc: 300\n",
      "doc: 310\n",
      "doc: 320\n",
      "doc: 330\n",
      "doc: 340\n",
      "doc: 350\n",
      "doc: 360\n",
      "doc: 370\n",
      "doc: 380\n",
      "doc: 390\n",
      "doc: 400\n",
      "doc: 410\n",
      "doc: 420\n",
      "doc: 430\n",
      "doc: 440\n",
      "doc: 450\n",
      "doc: 460\n",
      "doc: 470\n",
      "doc: 480\n",
      "doc: 490\n",
      "doc: 500\n",
      "doc: 510\n",
      "doc: 520\n",
      "doc: 530\n",
      "doc: 540\n",
      "doc: 550\n",
      "doc: 560\n",
      "doc: 570\n",
      "doc: 580\n",
      "doc: 590\n",
      "doc: 600\n",
      "doc: 610\n",
      "doc: 620\n",
      "doc: 630\n",
      "doc: 640\n",
      "doc: 650\n",
      "doc: 660\n",
      "doc: 670\n",
      "doc: 680\n",
      "doc: 690\n",
      "doc: 700\n",
      "doc: 710\n",
      "doc: 720\n",
      "doc: 730\n",
      "doc: 740\n",
      "doc: 750\n",
      "doc: 760\n",
      "doc: 770\n",
      "doc: 780\n",
      "doc: 790\n",
      "doc: 800\n",
      "doc: 810\n",
      "doc: 820\n",
      "doc: 830\n",
      "doc: 840\n",
      "doc: 850\n",
      "doc: 860\n",
      "doc: 870\n",
      "doc: 880\n",
      "doc: 890\n",
      "doc: 900\n",
      "doc: 910\n",
      "doc: 920\n",
      "doc: 930\n",
      "doc: 940\n",
      "doc: 950\n",
      "doc: 960\n",
      "doc: 970\n",
      "doc: 980\n",
      "doc: 990\n",
      "doc: 1000\n",
      "doc: 1010\n",
      "doc: 1020\n",
      "doc: 1030\n",
      "doc: 1040\n",
      "doc: 1050\n",
      "doc: 1060\n",
      "doc: 1070\n",
      "doc: 1080\n",
      "doc: 1090\n",
      "doc: 1100\n",
      "doc: 1110\n",
      "doc: 1120\n",
      "doc: 1130\n",
      "doc: 1140\n",
      "doc: 1150\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "start_time = time.time()\n",
    "def preprocess_documents_nltk(documents):\n",
    "    all_sentences = []\n",
    "    all_sentences_to_doc_map = []\n",
    "    count = 1\n",
    "    for doc in documents:\n",
    "        if count % 10 == 0:\n",
    "            print(\"doc:\",count)\n",
    "        # print(doc[\"acl_id\"])\n",
    "        # full_text = f\"{doc.get('full_text')}\"\n",
    "        full_text = \"\"\n",
    "        if doc[\"title\"] is not None: full_text = f'{doc[\"title\"]}.' \n",
    "        if doc[\"abstract\"] is not None: full_text = f'{full_text} {doc[\"abstract\"]}.' \n",
    "        if doc[\"full_text\"] is not None: full_text = f'{full_text} {doc[\"full_text\"]}.' \n",
    "        sentences = sent_tokenize(full_text)\n",
    "        for sentence in sentences:\n",
    "            all_sentences.append(sentence)\n",
    "            all_sentences_to_doc_map.append(doc[\"acl_id\"])\n",
    "        count += 1\n",
    "    return all_sentences,all_sentences_to_doc_map\n",
    "all_sentences, sentence_to_doc_map = preprocess_documents_nltk(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20497fba-dd01-4983-bf39-282f6604376f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings created.\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings\n",
    "embeddings = model.encode(all_sentences)\n",
    "print(\"Embeddings created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b652d14b-6c8f-4734-84e9-8fb734c71d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize KNN\n",
    "knn = NearestNeighbors(n_neighbors=2, metric='cosine')\n",
    "\n",
    "# Fit the model\n",
    "knn.fit(embeddings)\n",
    "\n",
    "# Example queries from ground truth\n",
    "queries = [doc[\"q\"] for doc in ground_truth_docs]\n",
    "query_embeddings = model.encode(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bdbd7e9e-9ab7-4e17-a853-406a09565aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract ground truth document IDs\n",
    "ground_truth = [doc[\"r\"][0] for doc in ground_truth_docs]  # Assuming single correct document per query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9b6e9d38-9c05-4958-9ddb-ddec7afd1fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform KNN search\n",
    "knn_indices_list = []\n",
    "knn_distances_list = []\n",
    "for query_embedding in query_embeddings:\n",
    "    distances, indices = knn.kneighbors([query_embedding])\n",
    "    knn_indices_list.append(indices[0].tolist())\n",
    "    knn_distances_list.append(distances[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "91f647f5-9d5d-4160-b2e7-48ac3167c88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built.\n"
     ]
    }
   ],
   "source": [
    "# Initialize FAISS index\n",
    "embedding_dim = embeddings.shape[1]\n",
    "faiss_index = faiss.IndexFlatL2(embedding_dim)\n",
    "\n",
    "# Add embeddings to FAISS index\n",
    "faiss_index.add(embeddings)\n",
    "print(\"FAISS index built.\")\n",
    "\n",
    "# Perform FAISS search\n",
    "faiss_indices_list = []\n",
    "faiss_distances_list = []\n",
    "for query_embedding in query_embeddings:\n",
    "    distances, indices = faiss_index.search(np.array([query_embedding]), 2)\n",
    "    faiss_indices_list.append(indices[0].tolist())\n",
    "    faiss_distances_list.append(distances[0].tolist())\n",
    "\n",
    "# Convert FAISS indices to document IDs\n",
    "faiss_predictions = [[sentence_to_doc_map[idx] for idx in indices] for indices in faiss_indices_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "62b177a6-8126-4c57-a9b9-3dab67fcd423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert KNN indices to document IDs\n",
    "knn_predictions = [[sentence_to_doc_map[idx] for idx in indices] for indices in knn_indices_list]\n",
    "\n",
    "# Calculate metrics\n",
    "def calculate_metrics(ground_truth, predictions):\n",
    "    flattened_ground_truth = [gt for gt in ground_truth for _ in range(2)]\n",
    "    flattened_predictions = [pred for sublist in predictions for pred in sublist]\n",
    "    precision = precision_score(flattened_ground_truth, flattened_predictions, average='macro')\n",
    "    recall = recall_score(flattened_ground_truth, flattened_predictions, average='macro')\n",
    "    f1 = f1_score(flattened_ground_truth, flattened_predictions, average='macro')\n",
    "    micro_precision = precision_score(flattened_ground_truth, flattened_predictions, average='micro')\n",
    "    micro_recall = recall_score(flattened_ground_truth, flattened_predictions, average='micro')\n",
    "    micro_f1 = f1_score(flattened_ground_truth, flattened_predictions, average='micro')\n",
    "    map_score = average_precision_score(\n",
    "        [1 if gt == pred else 0 for gt, pred in zip(flattened_ground_truth, flattened_predictions)], \n",
    "        [1] * len(flattened_predictions)\n",
    "    )\n",
    "    return precision, recall, f1, micro_precision, micro_recall, micro_f1, map_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7b200aff-ab22-43c1-b501-4fa77d35801a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marko/virt/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/marko/virt/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/marko/virt/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/marko/virt/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "knn_metrics = calculate_metrics(ground_truth, knn_predictions)\n",
    "faiss_metrics = calculate_metrics(ground_truth, faiss_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5ffe5806-8242-49eb-9347-bdd10dbaaabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Results:\n",
      "Query: Which versions of the Morfessor tokenizer have been proposed in the literature?\n",
      "Document ID: 2020.lrec-1.486, Sentence: While Morfessor 2.0 only implements force splitting certain characters to single-character morphs, i.e., Distance: 0.35286855697631836\n",
      "Document ID: E14-2006, Sentence: A number of Morfessor variants have been developed later, including Morfessor Categories-MAP (Creutz and Lagus, 2005a) and Allomorfessor (Virpioja et al., 2010) ., Distance: 0.3605763912200928\n",
      "Query: What is Dynamic Programming Encoding?\n",
      "Document ID: P97-1038, Sentence: As Chen (1993) points out, dynamic programming is particularly susceptible to deletions occurring in one of the two languages., Distance: 0.41305971145629883\n",
      "Document ID: 2020.cl-2.5, Sentence: This analysis provides important insights about the process of encoding and its applicability in other tasks., Distance: 0.42070674896240234\n",
      "Query: How many variants of the byte-pair encoding subword tokenizer have there been proposed?\n",
      "Document ID: 2022.tacl-1.5, Sentence: Related Work Improvements to Subword Tokenization Further improvements to standard subword tokenization like Byte Pair Encoding (BPE) (Sennrich et al., 2016) , WordPiece (Wu et al., 2016) , and SentencePiece (Kudo and Richardson, 2018) have been proposed., Distance: 0.19363439083099365\n",
      "Document ID: 2021.emnlp-demo.4, Sentence: For transformer models, Byte-Pair Encoding (BPE) (Sennrich et al.,  2016) and SentencePiece (Kudo and Richardson,  2018) are popular subword tokenizers., Distance: 0.2137002944946289\n",
      "Query: Can I do topic modelling with embeddings?\n",
      "Document ID: P16-1063, Sentence: In our model, topics are represented by embedding vectors, and are shared across documents., Distance: 0.23394852876663208\n",
      "Document ID: P16-1063, Sentence: In our model, topics are represented by embedding vectors, and are shared across documents., Distance: 0.23394852876663208\n",
      "Query: Which paper invented additive attention?\n",
      "Document ID: D17-1151, Sentence: Attention Mechanism The two most commonly used attention mechanisms are the additive (Bahdanau et al., 2015) variant, equation ( 6 ) below, and the computationally less expensive multiplicative variant (Luong et al., 2015a) , equation ( 7 ) below., Distance: 0.38030266761779785\n",
      "Document ID: 2023.ranlp-1.121, Sentence: We used Loung attention mechanism., Distance: 0.4318418502807617\n",
      "Query: Which variants of BERT (Bidirectional Encoding Representations from Transformers) are known?\n",
      "Document ID: 2022.naacl-main.156, Sentence: BERT (Bidirectional Encoder Representations from Transformers)., Distance: 0.08890128135681152\n",
      "Document ID: 2021.ranlp-1.147, Sentence: Recently the Bidirectional Encoder Representations from Transformers (BERT) model and its variants have gained enormous popularity due to its pretraining capability (Devlin et al., 2018) ., Distance: 0.14151573181152344\n",
      "Query: Of what concept are the tokenizers byte-pair encoding dropout (BPE-dropout) and unigram language model (ULM) an example?\n",
      "Document ID: 2020.acl-main.170, Sentence: In contrast to BPE, nearest neighbours of a token in the embedding space of BPE-dropout are often tokens that share sequences of characters with the original token., Distance: 0.3335789442062378\n",
      "Document ID: P18-1007, Sentence: BPE and the unigram language model share the same idea that they encode a text using fewer bits with a certain data compression principle (dictionary vs. entropy)., Distance: 0.3425341844558716\n",
      "Query: Which subword tokenizers can be trained with semi-supervised data?\n",
      "Document ID: 2021.emnlp-demo.4, Sentence: We use publicly available implementations of both subword tokenizers., Distance: 0.219363272190094\n",
      "Document ID: 2022.tacl-1.5, Sentence: While recent tokenization approaches based on data-derived subword lexicons are less brittle than manually engineered tokenizers, these techniques are not equally suited to all languages, and the use of any fixed vocabulary may limit a model's ability to adapt., Distance: 0.2532515525817871\n",
      "Query: What is an open vocabulary?\n",
      "Document ID: W17-4706, Sentence: The vocabulary is more open in that respect., Distance: 0.2789585590362549\n",
      "Document ID: W18-1207, Sentence: But in reality, the vocabulary of a natural language is open., Distance: 0.29430627822875977\n",
      "Query: What word categories have a potentially transparent translation, such that a segmentation into appropriate subword units is sufficient to learn transparent translations?\n",
      "Document ID: P16-1162, Sentence: Our hypothesis is that a segmentation of rare words into appropriate subword units is sufficient to allow for the neural translation network to learn transparent translations, and to generalize this knowledge to translate and produce unseen words., Distance: 0.2446286678314209\n",
      "Document ID: P16-1162, Sentence: Word categories whose translation is potentially transparent include: • named entities., Distance: 0.28600090742111206\n",
      "Query: Who invented the WordPiece tokenizer?\n",
      "Document ID: 2020.findings-emnlp.138, Sentence: • WordPiece., Distance: 0.3462631106376648\n",
      "Document ID: 2021.wanlp-1.20, Sentence: The same wordpiece vocabulary from ARABERTv0.2 was used for tokenization., Distance: 0.37389397621154785\n",
      "Query: What is vocabulary clustering?\n",
      "Document ID: 2020.lrec-1.612, Sentence: We use it to cluster the word embbeddings of the dataset vocabulary., Distance: 0.2560526728630066\n",
      "Document ID: R09-1075, Sentence: Clustering is an effect of the semantic association., Distance: 0.2644389867782593\n",
      "Query: Is it a good idea to compare BLEU scores across papers?\n",
      "Document ID: W18-6319, Sentence: Together, these issues make it difficult to evaluate and compare BLEU scores across papers, which impedes comparison and replication., Distance: 0.16124004125595093\n",
      "Document ID: W18-6319, Sentence: These parameters are often not reported or are hard to find, and consequently, BLEU scores between papers cannot be directly compared., Distance: 0.18723762035369873\n",
      "Query: Can you describe the phenomenon of non-concatenative morphology?\n",
      "Document ID: 2021.findings-emnlp.60, Sentence: Non-Concatenative Morphology The most interesting results can be seen for the two non-concatenative morphological phenomena: vowel harmony and reduplication., Distance: 0.15792423486709595\n",
      "Document ID: 2021.findings-emnlp.60, Sentence: However, our results do show a clear gap between the models' competence for non-concatenative and concatenative morphology., Distance: 0.21210438013076782\n",
      "Query: Is there an optimal number of BPE merge operations for transformers?\n",
      "Document ID: W19-6620, Sentence: Based on the findings, we make the following recommendations for selecting BPE merge operations in the future: • For Transformer-based architectures, we recommend the sweep be concentrated in the 0 − 4k range., Distance: 0.20099246501922607\n",
      "Document ID: W19-6620, Sentence: For our main experiments, we can already see a pretty consistent trend that for deep-transformer architecture, 0.5k and 32k merge operations always roughly correspond to the best and worst BPE configurations, respectively., Distance: 0.24508434534072876\n",
      "Query: What do we know about the anisotropy of contextualised embeddings?\n",
      "Document ID: D19-1006, Sentence: Given that isotropy has both theoretical and empirical benefits for static embeddings (Mu et al., 2018) , the extent of anisotropy in contextualized represen-tations is surprising., Distance: 0.20384585857391357\n",
      "Document ID: D19-1006, Sentence: We ultimately found that after adjusting for anisotropy, on average, less than 5% of the variance in a word's contextualized representations could be explained by a static embedding., Distance: 0.22481876611709595\n",
      "Query: What are common features between S-BERT, SpanBERT and PhraseBERT?\n",
      "Document ID: 2020.coling-main.609, Sentence: 5.4 BERT vs. CharacterBERT: How Significant Is the Difference?, Distance: 0.2403700351715088\n",
      "Document ID: 2020.coling-main.609, Sentence: Figure 5 shows that CharacterBERT often improves over a vanilla BERT., Distance: 0.2747451066970825\n",
      "Query: Can you build a tokenizer using the theory of optimal transport?\n",
      "Document ID: 2023.emnlp-main.614, Sentence: Tokenization is an active area of research and various solutions based on data balancing (Johnson et al., 2017; Conneau and Lample, 2019) , optimal transport (Xu et al., 2021) , fuzzy subwords (Provilkov et al., 2020) , and many more (Chung et al., 2020; Tay et al., 2022) have been proposed., Distance: 0.32883620262145996\n",
      "Document ID: 2021.acl-long.571, Sentence: Intuitively, optimal transport is about finding the best transporting mass from the char distribution to the target token distribution with the minimum work defined by P , D ., Distance: 0.3346719741821289\n",
      "Query: Which character-based transformers have been proposed?\n",
      "Document ID: 2022.findings-acl.194, Sentence: Character-level modeling with Transformers appears to be more difficult., Distance: 0.24994933605194092\n",
      "Document ID: 2022.semeval-1.122, Sentence: 2018) for Transformers., Distance: 0.3424062132835388\n",
      "Query: What is the difference between ULM (Unigram LM) and SentencePiece?\n",
      "Document ID: W19-5005, Sentence: Somewhat surprisingly, the model trained on sentence embeddings outperformed the fine-tuned ULMFiT., Distance: 0.40698331594467163\n",
      "Document ID: 2020.findings-emnlp.414, Sentence: Since the unigram LM method selects tokens during vocabulary construction using a global optimization procedure, it does not produce junk tokens; this property also allows it to avoid merging frequent tokens with their neighbors too aggressively., Distance: 0.41795313358306885\n",
      "Query: Do we have systems that automatically split compounds into their constituents?\n",
      "Document ID: E03-1076, Sentence: Related Work While the linguistic properties of compounds are widely studied [Langer, 1998] , there has been only limited work on empirical methods to split up compounds for specific applications., Distance: 0.35336267948150635\n",
      "Document ID: E03-1076, Sentence: Splitting Options Compounds are created by joining existing words together., Distance: 0.3725854158401489\n",
      "Query: Which papers introduced transition-based dependency parsing?\n",
      "Document ID: J11-1007, Sentence: Of particular note is past work on combining graph-based and transition-based dependency parsers., Distance: 0.1505224108695984\n",
      "Document ID: C12-1059, Sentence: Introduction The basic idea in transition-based dependency parsing is to define a nondeterministic transition system for mapping sentences to dependency trees and to perform parsing as search for the optimal transition sequence for a given sentence (Nivre, 2008) ., Distance: 0.1594628095626831\n",
      "Query: Which papers introduced graph-based dependency parsing?\n",
      "Document ID: J11-1007, Sentence: Of particular note is past work on combining graph-based and transition-based dependency parsers., Distance: 0.13747954368591309\n",
      "Document ID: K18-2022, Sentence: We propose a dependency parsing model based on the graph-based parser by Dozat and Manning (2016) ., Distance: 0.17296695709228516\n",
      "Query: Has there been an analysis comparing classic transition-based and graph-based dependency parsers?\n",
      "Document ID: J11-1007, Sentence: Of particular note is past work on combining graph-based and transition-based dependency parsers., Distance: 0.07281941175460815\n",
      "Document ID: J11-1007, Sentence: As already noted, there are several recent developments in data-driven dependency parsing, which can be seen as targeting the specific weaknesses of traditional graphbased and transition-based models, respectively., Distance: 0.14937257766723633\n",
      "Query: What are some benchmarks that can measure the syntactic knowledge of a language model?\n",
      "Document ID: 2020.acl-demos.10, Sentence: Unlike broad-coverage language modeling metrics such as perplexity, these evaluations are targeted to reveal whether models have learned specific knowledge about the syntactic structure of language (see e.g., Distance: 0.2836970090866089\n",
      "Document ID: D18-1151, Sentence: We hope that our data set, and future extensions to other phenomena and languages, will make it possible to measure progress in syntactic language modeling and will lead to better understanding of the syntactic generalizations captured by language models., Distance: 0.2879500389099121\n",
      "Query: What is a common architecture for neural dependency parsing?\n",
      "Document ID: N18-1037, Sentence: We start by describing our base dependency parsing model, which is neural network based and performs among the state-of-theart., Distance: 0.17780572175979614\n",
      "Document ID: D15-1158, Sentence: Recently, progress in dependency parsing has been made by introducing non-linear, neuralnetwork based models (Pei et al., 2015; Chen and Manning, 2014; Weiss et al., 2015; Dyer et al., 2015; Zhou et al., 2015) ., Distance: 0.1964232325553894\n",
      "Query: What system first incorporated adapters for cross-lingual dependency parsing?\n",
      "Document ID: 2020.emnlp-main.180, Sentence: Cross-Lingual Dependency Parsing The availability of consistent dependency treebanks in many languages (McDonald et al., 2013; Nivre et al., 2018) has provided an opportunity for the study of cross-lingual parsing., Distance: 0.2023695707321167\n",
      "Document ID: 2021.acl-long.560, Sentence: Cross-lingual dependency parsing The new language., Distance: 0.20790040493011475\n",
      "Query: How can I probe for dependency structure in a model's representations?\n",
      "Document ID: W02-0603, Sentence: However, there is still room for considerable improvement in the model structure, especially regarding the representation of contextual dependencies., Distance: 0.33307021856307983\n",
      "Document ID: P04-1085, Sentence: contextual dependencies discussed in the previous subsection, we must augment the structure of our model to obtain a more general one., Distance: 0.3690367341041565\n",
      "Query: What is an alternative to UD for dependency annotation?\n",
      "Document ID: W18-6008, Sentence: Introduction Universal Dependencies (UD) is an astonishing collaborative project of dozens of research groups around the world, developing an annotation scheme that is applicable to all languages and proposing treebanks based on that scheme for more than 70 languages from different language families (Nivre et al., Distance: 0.21647405624389648\n",
      "Document ID: 2023.eacl-main.76, Sentence: Universal Dependencies Universal Dependencies (UD) (Nivre et al., 2016 (Nivre et al., , 2020;; de Marneffe et al., 2021 ) is an initiative focused on the development of dependency treebanks., Distance: 0.22202438116073608\n",
      "Query: What are some common issues associated with the NLI task?\n",
      "Document ID: U14-1020, Sentence: An overview of the shared task results and a review of prior NLI work can be found in Tetreault et al., Distance: 0.23309993743896484\n",
      "Document ID: U14-1020, Sentence: Background NLI is a fairly recent, but rapidly growing area of research., Distance: 0.2551560401916504\n",
      "Query: Where can I find a discussion of the probing paradigm, including its shortcomings and disadvantages?\n",
      "Document ID: 2020.findings-emnlp.389, Sentence: Moreover, it would be interesting to investigate alternative probing strategies in order to better disentangle what pertains to the model itself from what is specific to a given probing strategy., Distance: 0.4030197858810425\n",
      "Document ID: 2020.lrec-1.130, Sentence: (4) Probes/Challenges: turns that questioned or disagreed with a prior idea., Distance: 0.42735183238983154\n",
      "Query: What is the \"attention is not explanation\" debate?\n",
      "Document ID: D19-1002, Sentence: A recent paper claims that 'Attention is not Explanation' (Jain and Wallace, 2019)., Distance: 0.1455138921737671\n",
      "Document ID: D19-1002, Sentence: A recent paper claims that 'Attention is not Explanation' (Jain and Wallace, 2019)., Distance: 0.1455138921737671\n",
      "Query: What is \"faithfulness\" in the context of interpretability?\n",
      "Document ID: 2020.acl-main.386, Sentence: Defining Faithfulness What does it mean for an interpretation method to be faithful?, Distance: 0.1238260269165039\n",
      "Document ID: 2020.acl-main.386, Sentence: To our knowledge, the term \"faithful interpretability\" was coined in Harrington et al., Distance: 0.17989766597747803\n",
      "Query: What are some studies that compare saliency methods in NLP?\n",
      "Document ID: 2021.naacl-main.399, Sentence: To address this question, we conduct a comprehensive and quantitative evaluation of saliency methods on a fundamental category of NLP models: neural language models., Distance: 0.20249134302139282\n",
      "Document ID: 2021.naacl-main.399, Sentence: To address this question, we conduct a comprehensive and quantitative evaluation of saliency methods on a fundamental category of NLP models: neural language models., Distance: 0.20249134302139282\n",
      "Query: What is a recurrent neural network grammar?\n",
      "Document ID: N16-1024, Sentence: Recurrent Neural Network Grammars., Distance: 0.1823408007621765\n",
      "Document ID: 2021.naacl-main.132, Sentence: 2019 ) is a recurrent neural network that explicitly models recursive structure through memory writing and erasing operations., Distance: 0.24463844299316406\n",
      "Query: What tasks comprise the GLUE suite?\n",
      "Document ID: 2020.acl-main.329, Sentence: With these in mind, our choice of tasks and datasets for GLUE-CoS are based on the following principles : • We choose a variety of tasks, ranging from simpler ones, on which the research community has already achieved high accuracies, to relatively more complex, on which very few attempts have been made., Distance: 0.296653687953949\n",
      "Document ID: 2021.acl-long.260, Sentence: We leave the training details for downstream tasks and experiments on GLUE benchmark (Wang et al., 2018) in the appendix., Distance: 0.31215155124664307\n",
      "Query: What is item response theory? Has it been applied to NLP?\n",
      "Document ID: 2021.acl-long.92, Sentence: 2 Item Response Theory Baker and Kim (1993) introduce Item Response Theory (IRT), a statistical framework to measure the probability of a responder (human or AI system) predicting a correct answer for a given item (test example)., Distance: 0.23685693740844727\n",
      "Document ID: 2021.acl-long.346, Sentence: Item Response Theory (Lord et al., 1968; Baker, 2001 , IRT, reviewed in §2), a widely used (van Rijn et al., 2016) alternative in educational testing to simple summary statistics (Edgeworth, 1888) ., Distance: 0.26297450065612793\n",
      "Query: Why is the Japanese GSD treebank so easy to parse?\n",
      "Document ID: 2023.eacl-main.76, Sentence: Interestingly, the second and third-highest CONF treebanks are both Japanese: GSDLUW (0.93, 0.07, 130,298) and GSD (0.93, 0.07, 168, 333) ., Distance: 0.29524505138397217\n",
      "Document ID: W06-2920, Sentence: Another reason are the various annotation schemes and logical data formats used by different treebanks, which make it tedious to apply a parser to many treebanks., Distance: 0.30972176790237427\n",
      "Query: What is Targeted Syntactic Evaluation?\n",
      "Document ID: D18-1151, Sentence: Targeted Syntactic Evaluation of Language Models., Distance: 0.22621101140975952\n",
      "Document ID: 2020.acl-demos.10, Sentence: The targeted syntactic evaluation paradigm (Linzen et al., 2016; Lau et al., 2017; Gulordava et al., 2018; Marvin and Linzen, 2018; Futrell et al., 2019; Warstadt et al., 2020) incorporates methods from psycholinguistic experiments, designing sentences which hold most lexical and syntactic features of each sentence constant while minimally varying features that determine grammaticality or surprise characteristics of the sentence., Distance: 0.2532760500907898\n",
      "Query: What are negative polarity items?\n",
      "Document ID: 2020.tacl-1.54, Sentence: A study of negative polarity items (NPIs) by Warstadt et al., Distance: 0.1625182032585144\n",
      "Document ID: D18-1151, Sentence: Negative polarity items Negative polarity items, introduced in example (2) above, are words that (to a first approximation) need to occur in the context of negation., Distance: 0.20142924785614014\n",
      "Query: Are NLP models like BERT sensitive to word order? \n",
      "Document ID: 2021.emnlp-main.230, Sentence: Ettinger (2020) recently observed that BERT accuracy decreases for some word order perturbed examples, but not for others., Distance: 0.254067063331604\n",
      "Document ID: P19-1334, Sentence: Its performance particularly stood out for the lexical overlap cases, suggesting that some of BERT's success at MNLI may be due to a greater tendency to incorporate word order information compared to other models., Distance: 0.2731855511665344\n",
      "Query: What are common syntactic annotation schemes?\n",
      "Document ID: P13-1023, Sentence: Syntactic annotation schemes come in many forms, from lexical categories such as POS tags to intricate hierarchical structures., Distance: 0.1793358325958252\n",
      "Document ID: P13-1023, Sentence: We view this as a major advantage of semantic annotation schemes over their syntactic counterparts, especially given the huge amount of manual labor required for large syntactic annotation projects., Distance: 0.2082521915435791\n",
      "Query: What are common semantic annotation schemes?\n",
      "Document ID: W05-1714, Sentence: • What types of annotation schemes and formats are applied?, Distance: 0.22063839435577393\n",
      "Document ID: W14-5149, Sentence: Many such annotations schemas have been proposed for different NLP applications., Distance: 0.2413240671157837\n",
      "Query: What are universals?\n",
      "Document ID: 2021.cl-2.11, Sentence: Universal Dependencies., Distance: 0.38530033826828003\n",
      "Document ID: J19-3005, Sentence: Using such universals, features can be deduced by modus ponens., Distance: 0.41393613815307617\n",
      "Query: Which databased can I use to analyse typological information?\n",
      "Document ID: J19-3005, Sentence: In fact, typological database documentation is incomplete, approximate, and discrete., Distance: 0.3086501359939575\n",
      "Document ID: J19-3005, Sentence: (ii) What are the advantages and limitations of currently available typological databases?, Distance: 0.3189281225204468\n",
      "Query: What is the purpose of language embeddings?\n",
      "Document ID: 2021.acl-long.560, Sentence: language embeddings., Distance: 0.23210930824279785\n",
      "Document ID: 2023.nodalida-1.67, Sentence: Further analysis has shown that the nature of the underlying data used to generate language embeddings can have a significant impact on what features are encapsulated (Bjerva and Augenstein, 2018a,b; Bjerva et al., 2019b) , and even that such representations contain typological generalisations ( Östling and Kurfalı, 2023) ., Distance: 0.26064813137054443\n",
      "Query: What technique can be used to try to explain the inner workings of a language model?\n",
      "Document ID: W18-5424, Sentence: Do Language Models Understand Anything?, Distance: 0.2988640069961548\n",
      "Document ID: 2020.emnlp-main.263, Sentence: There also exist explainability methods that generate textual explanations (Camburu et al., 2018) and are trained posthoc or jointly with the model at hand., Distance: 0.3513261079788208\n",
      "Query: What is weak supervision?\n",
      "Document ID: 2023.acl-long.796, Sentence: Related work Weak supervision., Distance: 0.16960299015045166\n",
      "Document ID: 2023.acl-long.796, Sentence: However, in a broader context, weak supervision can have different forms., Distance: 0.17221224308013916\n",
      "Query: What is the name of the first Dutch transformer-based pre-trained language model?\n",
      "Document ID: 2020.findings-emnlp.292, Sentence: The transformer model led to other popular language models, e.g., Distance: 0.25250935554504395\n",
      "Document ID: 2020.emnlp-main.263, Sentence: For the Transformer model, we fine-tune the pre-trained basic, uncased language model (LM) (Wolf et al., 2019) ., Distance: 0.255634069442749\n",
      "Query: What is BERTology?\n",
      "Document ID: 2020.tacl-1.54, Sentence: A Primer in BERTology: What We Know About How BERT Works., Distance: 0.2430298924446106\n",
      "Document ID: 2021.eacl-main.270, Sentence: Syntactic BERTology., Distance: 0.27290403842926025\n",
      "Query: What is the name of the research initiative creating resources for African languages?\n",
      "Document ID: 2023.acl-long.609, Sentence: First, it is the second most available resource after the religious domain for most African languages., Distance: 0.30357927083969116\n",
      "Document ID: 2021.gem-1.10, Sentence: Chris Emezue and Rubungo Andre Niyongabo explored potential low-resource African languages for crowdsourcing., Distance: 0.33035099506378174\n",
      "Query: What is the WMT conference about?\n",
      "Document ID: 2023.wmt-1.0, Sentence: This is the eighth time WMT has been held as a conference., Distance: 0.20875990390777588\n",
      "Document ID: 2023.wmt-1.0, Sentence: This is the eighth time WMT has been held as a conference., Distance: 0.20875990390777588\n",
      "Query: How many tasks did SemEval 2022 have?\n",
      "Document ID: 2022.semeval-1.0, Sentence: The tasks for SemEval-2022 were proposed in 2021, and next year's tasks have already been selected and are underway.., Distance: 0.2245805263519287\n",
      "Document ID: 2022.semeval-1.0, Sentence: Introduction Welcome to SemEval-2022!, Distance: 0.3598020076751709\n",
      "Query: What is the modular and parameter-efficient alternative to fine-tuning called?\n",
      "Document ID: 2023.acl-long.796, Sentence: AdapterHub (Pfeiffer et al., 2020) is used for implementing parameter-efficient fine-tuning., Distance: 0.22487258911132812\n",
      "Document ID: 2023.emnlp-demo.13, Sentence: First, parameter-efficient fine-tuning (Lialin et al., 2023; Sabry and Belz, 2023) focuses on the aspect of computational efficiency and feasibility by only fine-tuning a small * Authors contributed equally., Distance: 0.2417759895324707\n",
      "Query: In what task does post-editing play a role?\n",
      "Document ID: 2022.emnlp-main.532, Sentence: Post-editing., Distance: 0.2635999917984009\n",
      "Document ID: 2013.mtsummit-wptp.5, Sentence: Tasks The users performed the post-editing tasks using a portal that was especially developed for postediting, the interface of which is displayed in -For example, try to rectify word order and spelling when they are inappropriate to the extent that the text has become impossible or difficult to comprehend., Distance: 0.30159395933151245\n",
      "Query: Are certain researchers critical of parsing?\n",
      "Document ID: J11-1007, Sentence: Other studies have tried to overcome the weaknesses of parsing models by changing the underlying model structure directly., Distance: 0.29493165016174316\n",
      "Document ID: W02-0905, Sentence: In terms of parsing development, it is broadly assumed that parsers need such information in order to reduce the number of possible analyses and, therefore, solve syntactic ambiguity., Distance: 0.29524362087249756\n",
      "Query: What measure has been proposed to stimulate research in multilingual NLP?\n",
      "Document ID: N19-1367, Sentence: More generally, multilingual NLP is an active and growing area of research., Distance: 0.14542710781097412\n",
      "Document ID: J19-3005, Sentence: In this section we discuss potential future research avenues that may result in a closer and more effective integration of linguistic typology and multilingual NLP., Distance: 0.19934797286987305\n",
      "Query: What dataset can I use to test multilingual information retrieval?\n",
      "Document ID: 2023.mrl-1.21, Sentence: Our dataset is of a scale to accommodate both large-scale training and evaluation of multilingual retrieval methods., Distance: 0.19728344678878784\n",
      "Document ID: 2023.mrl-1.21, Sentence: While progress towards multilingual retrieval through the release of datasets such as Mr. TYDI (Zhang et al., 2021) and mMARCO (Bonifacio et al., 2021) , both are limited in that they evaluate monolingual retrieval for a range of languages, rather than true multilingual retrieval, using multiple languages simultaneously., Distance: 0.21849697828292847\n",
      "Query: Is the impact of big tech on NLP growing?\n",
      "Document ID: 2023.acl-long.734, Sentence: Overall, the presence of Big Tech companies in NLP research has contributed to the growth and advancement of the field., Distance: 0.1467342972755432\n",
      "Document ID: 2023.acl-long.734, Sentence: In turn, we are observing a greater presence of large technology companies (Big Tech) on NLP research than ever before., Distance: 0.18295705318450928\n",
      "Query: What are drawbacks of character-level machine translation?\n",
      "Document ID: 2022.findings-acl.194, Sentence: We speculate that the main reasons for not considering character-level modeling are its lower efficiency and the fact that the literature shows no clear improvement of translation quality., Distance: 0.2440134882926941\n",
      "Document ID: 2022.findings-acl.194, Sentence: We present a literature and empirical survey that critically assesses the state of the art in character-level modeling for machine translation (MT)., Distance: 0.2701148986816406\n",
      "Query: What is the BabyLM Challenge?\n",
      "Document ID: 2023.conll-babylm.1, Sentence: The BabyLM Challenge is a shared task that invites * Equal contribution., Distance: 0.2118714451789856\n",
      "Document ID: 2023.conll-babylm.1, Sentence: (2023) Future BabyLM Challenges The first iteration of the BabyLM Challenge yielded many successes, but also some organizational and scientific challenges., Distance: 0.2444542646408081\n",
      "Query: What does preregistration mean in research?\n",
      "Document ID: 2021.naacl-main.51, Sentence: Preregistration refers to the practice of specifying what you are going to do, and what you expect to find in your study, before carrying out the study., Distance: 0.11686408519744873\n",
      "Document ID: 2021.naacl-main.51, Sentence: How does preregistration work?, Distance: 0.1977977156639099\n",
      "Query: How can a transition-based parser deal with non-projectivity?\n",
      "Document ID: N16-1024, Sentence: Constraints on parser transitions., Distance: 0.16402101516723633\n",
      "Document ID: Q15-1026, Sentence: Transition-based Parsing., Distance: 0.20211577415466309\n",
      "Query: How is the input represented in a delexicalized dependency parser?\n",
      "Document ID: P05-1013, Sentence: The parser builds dependency graphs by traversing the input from left to right, using a stack to store tokens that are not yet complete with respect to their dependents., Distance: 0.26450031995773315\n",
      "Document ID: D11-1006, Sentence: Multi-Source Transfer of Delexicalized Dependency Parsers., Distance: 0.2664937376976013\n",
      "Query: What are called parsing oracles that are non-deterministic?\n",
      "Document ID: Q13-1033, Sentence: Training Deterministic Parsers with Non-Deterministic Oracles., Distance: 0.18775522708892822\n",
      "Document ID: Q13-1033, Sentence: Dynamic oracles are non-deterministic in the sense that they return a set of valid transitions for a given parser state and gold tree., Distance: 0.18889260292053223\n",
      "Query: What are the four transitions of an arc-eager dependency parser?\n",
      "Document ID: J08-4003, Sentence: Arc-Eager Parsing The transition set T for the arc-eager, stack-based parser is defined in Figure 5 and contains four types of transitions: 1., Distance: 0.1807507872581482\n",
      "Document ID: J08-4003, Sentence: The arc-eager parser differs from the arc-standard one by attaching right dependents (using RIGHT-ARC e l transitions) as soon as possible, that is, before the right dependent has found all its right dependents., Distance: 0.20519602298736572\n",
      "Query: What languages did the CoNLL 2006 shared task include?\n",
      "Document ID: D07-1013, Sentence: on a variety of languages, as seen in Table 1 , which shows results for the two top performing systems in the CoNLL-X shared task, McDonald et al., Distance: 0.29773926734924316\n",
      "Document ID: C16-1147, Sentence: Some of these tasks featured languages other than English., Distance: 0.3211938738822937\n",
      "Query: What was the input of the CoNLL 2017 shared task?\n",
      "Document ID: K17-3001v2, Sentence: CoNLL 2017 has picked up the threads of those pioneering tasks and addressed these two issues., Distance: 0.24836444854736328\n",
      "Document ID: E17-1027, Sentence: We use the version of the data provided to the CoNLL 2016 Shared Task participants., Distance: 0.2985227704048157\n",
      "Query: What dataset can be used for multilingual natural language inference?\n",
      "Document ID: 2021.americasnlp-1.23, Sentence: For development and testing, we used parallel sentences belonging to a new natural language inference dataset for the 10 indigenous languages featured in our shared task, which is a manual translation of the Spanish version of the multilingual XNLI dataset (Conneau et al., 2018) ., Distance: 0.2604796886444092\n",
      "Document ID: 2022.naacl-main.125, Sentence: B Datasets We employ three datasets for text classification and three datasets for natural language inference as described below., Distance: 0.26372861862182617\n",
      "Query: What are visual dependency relations?\n",
      "Document ID: D13-1128, Sentence: Table 1 : Visual Dependency Grammar defines eight relations between pairs of annotated regions., Distance: 0.21041899919509888\n",
      "Document ID: D13-1128, Sentence: Visual Dependency Representation In analogy to dependency grammar for natural language syntax, we define Visual Dependency Grammar to describe the spatial relations between pairs of image regions., Distance: 0.2490817904472351\n",
      "Query: How can the digital status of languages be categorized?\n",
      "Document ID: 2020.acl-main.560, Sentence: The Six Kinds of Languages In order to summarize the digital status and 'richness' of languages in the context of data availability, we propose a taxonomy based on the number of language resources which exist for different languages., Distance: 0.30185210704803467\n",
      "Document ID: 2021.cl-2.11, Sentence: These categories are widely attested in the world's languages., Distance: 0.34142786264419556\n",
      "Query: What toolkit can be used to easily fine-tune contextualized embeddings in a multi-task setting?\n",
      "Document ID: 2021.eacl-demos.22, Sentence: In this paper we present MACHAMP, a toolkit for easy fine-tuning of contextualized embeddings in multi-task settings., Distance: 0.09540104866027832\n",
      "Document ID: 2021.eacl-demos.22, Sentence: In this paper we present MACHAMP, a toolkit for easy fine-tuning of contextualized embeddings in multi-task settings., Distance: 0.09540104866027832\n",
      "Query: What is the name of the statistical machine translation toolkit developed by Philip Koehn and colleagues?\n",
      "Document ID: D16-1162, Sentence: (2014) ) is a variant of statistical machine translation (SMT; Brown et al., Distance: 0.21581321954727173\n",
      "Document ID: P12-3008, Sentence: The Moses SMT toolkit (Koehn et al., 2007 ) provides a complete statistical translation system distributed under the LGPL license., Distance: 0.26813220977783203\n",
      "Query: What kind of encoder can be used to incorporate syntactic structure into a neural machine translation system?\n",
      "Document ID: D17-1209, Sentence: We present a simple and effective approach to incorporating syntactic structure into neural attention-based encoderdecoder models for machine translation., Distance: 0.27734673023223877\n",
      "Document ID: Q17-1004, Sentence: Inspired by the encoder-decoder framework for neural machine translation (Bahdanau et al., 2015; Cho et al., 2014) , we use an LSTM to capture full sentence features, and another LSTM to generate the constituent hierarchies for each word., Distance: 0.2954838275909424\n",
      "Query: Who developed ELMo?\n",
      "Document ID: 2021.eacl-main.179, Sentence: ELMo., Distance: 0.2725403904914856\n",
      "Document ID: 2020.lrec-1.223, Sentence: • ELMo., Distance: 0.30687201023101807\n",
      "Query: What is a diagnostic classifier?\n",
      "Document ID: 2020.acl-main.682, Sentence: The diagnostic classifier outputs d a logits where each of them models the probability P (y k = 1|d) (where d is dialogue state representation), one for each attribute y k to be predicted., Distance: 0.29962587356567383\n",
      "Document ID: 2020.cl-2.4, Sentence: (2018a) , we use diagnostic classifiers (Shi, Padhi, and Knight 2016; Adi et al., Distance: 0.31407785415649414\n",
      "Query: Can LSTMs learn syntactic dependencies?\n",
      "Document ID: Q16-1037, Sentence: Assessing the Ability of {LSTM}s to Learn Syntax-Sensitive Dependencies., Distance: 0.17011237144470215\n",
      "Document ID: Q16-1037, Sentence: Linguistic regularities are often sensitive to syntactic structure; can such dependencies be captured by LSTMs, which do not have explicit structural representations?, Distance: 0.18204981088638306\n",
      "Query: Which teams participated in the Americas 2021 shared task?\n",
      "Document ID: 2021.americasnlp-1.23, Sentence: Eight teams participated in the AmericasNLP 2021 Shared Task on OMT., Distance: 0.2748008370399475\n",
      "Document ID: W18-2409, Sentence: (Eq.9) 4 Participation in the Shared Task A total of six teams from eight different institutions participated in the NEWS 2018 Shared Task., Distance: 0.36466825008392334\n",
      "Query: Which team ranked first in the CoNLL 2018 shared task based on the LAS score\n",
      "Document ID: 2020.emnlp-main.180, Sentence: For evaluation, the official CoNLL 2018 Shared Task script 7 is used to obtain LAS scores on the test set of each treebank., Distance: 0.34808313846588135\n",
      "Document ID: J08-4003, Sentence: For comparison, we also include the results of the two top scoring systems in the CoNLL-X shared task, those of McDonald, Lerman, and Pereira (2006) and Nivre et al., Distance: 0.35582125186920166\n",
      "Query: Who developed Udify?\n",
      "Document ID: D19-1279, Sentence: Code for UDify is available at https:// github.com/hyperparticle/udify., Distance: 0.37907910346984863\n",
      "Document ID: D19-1279, Sentence: com/hyperparticle/udify., Distance: 0.3977568745613098\n",
      "Query: How can masked language models be evaluated intrinsically?\n",
      "Document ID: 2023.conll-babylm.1, Sentence: (2023) simplified the masked language modeling objective by coarse-graining the output classes, with little effect., Distance: 0.22423076629638672\n",
      "Document ID: 2020.acl-main.240, Sentence: Masked Language Model Scoring., Distance: 0.2408161163330078\n",
      "Query: What are some relevant benchmarks for code-mixed NLP?\n",
      "Document ID: 2021.gem-1.10, Sentence: Benchmarks in NLG In this section, we summarize common criticisms of benchmarks in NLP, discuss how they apply to NLG, and how we plan to address them., Distance: 0.27216219902038574\n",
      "Document ID: 2021.bppf-1.1, Sentence: In this talk, I will argue for three desired interrelated shifts in NLP benchmarking, which motivate and support each other, that should direct further research., Distance: 0.2901149392127991\n",
      "Query: What are some multilingual pretrained language models for Indic languages?\n",
      "Document ID: 2020.findings-emnlp.445, Sentence: For Indic languages, two such multilingual models are available: XLM-R (Conneau et al., 2020) and multilingual BERT (Devlin et al., 2019) ., Distance: 0.16149044036865234\n",
      "Document ID: 2023.acl-long.693, Sentence: Models Most multilingual pretrained language models and their variants like mBERT (Devlin et al., 2019) , mT5 (Xue et al., 2021) , and XLM (Conneau and Lample, 2019) are trained on major Indic languages., Distance: 0.17862123250961304\n",
      "Query: What are some multilingual pretrained models for African languages?\n",
      "Document ID: 2021.mrl-1.11, Sentence: We introduce AfriBERTa, a transformer-based multilingual language models trained on 11 African languages, all of which are low-resource., Distance: 0.19774401187896729\n",
      "Document ID: 2021.mrl-1.11, Sentence: Besides showing that multilingual language models are viable on low-resource African languages with small training data, we also introduce the first language models for four of these languages: Kinyarwanda, Kirundi, Nigerian Pidgin and Tigrinya., Distance: 0.20773720741271973\n",
      "Query: What are some datasets you can use to probe models for linguistic information? \n",
      "Document ID: 2023.findings-acl.185, Sentence: There are some datasets that come from speech transcription, news, dialogues, books, government documents, and treebanks., Distance: 0.3101806640625\n",
      "Document ID: 2020.findings-emnlp.445, Sentence: Datasets are available for some tasks for a few languages., Distance: 0.3153587579727173\n",
      "Query: Can you transliterate languages for better language modelling?\n",
      "Document ID: 2023.findings-eacl.50, Sentence: Does Transliteration Help Multilingual Language Modeling?., Distance: 0.13365477323532104\n",
      "Document ID: 2023.findings-eacl.50, Sentence: Transliteration in Language Modeling Different works have applied transliteration in different aspect for language models., Distance: 0.19044506549835205\n",
      "Query: What are some surveys on approaches to code-switched NLP?\n",
      "Document ID: 2023.findings-acl.185, Sentence: We hope this survey can encourage and lead NLP researchers in a better direction on code-switching research., Distance: 0.1655200719833374\n",
      "Document ID: 2023.findings-acl.185, Sentence: The Decades Progress on Code-Switching Research in NLP: A Systematic Survey on Trends and Challenges., Distance: 0.19243991374969482\n",
      "Query: What are some feature attribution metrics for language model interpretability \n",
      "Document ID: 2020.cl-2.4, Sentence: Interpretability studies have been one of the emerging trends in NLP as hinted by the on-going Representation Evaluation (RepEval) (Nangia et al., Distance: 0.3318890333175659\n",
      "Document ID: 2020.acl-main.386, Sentence: Explanations provided by \"inherently interpretable\" models must be held to the same standards as post-hoc interpretation methods, and be evaluated for faithfulness using the same set of evaluation techniques., Distance: 0.35817766189575195\n",
      "Query: What are natural language understanding benchmarks for Indic languages\n",
      "Document ID: 2020.findings-emnlp.445, Sentence: IndicGLUE: Multilingual NLU Benchmark We now introduce IndicGLUE, the Indic General Language Understanding Evaluation Benchmark, which is a collection of various NLP tasks as described below., Distance: 0.1900331974029541\n",
      "Document ID: 2020.findings-emnlp.445, Sentence: Such a benchmark is missing for Indic languages and the goal of this work is to fill this void., Distance: 0.19167697429656982\n",
      "Query: What is a toolkit for easy multi-task finetuning of models?\n",
      "Document ID: 2022.tacl-1.25, Sentence: Popular benchmarks usually aggregate multiple tasks to spur the progress of generalizable models., Distance: 0.3535197377204895\n",
      "Document ID: 2022.tacl-1.25, Sentence: The benchmarks usually aggregate multiple model-agnostic tasks under a unified framework, enabling researchers to fairly compare different models., Distance: 0.35661816596984863\n",
      "Query: How much data is required to adapt a BERT model to another language of the same family?\n",
      "Document ID: 2021.findings-acl.433, Sentence: In-stead, monolingual BERT models are transferable to languages with very little data if the source and target languages are relatively similar., Distance: 0.24875634908676147\n",
      "Document ID: 2022.lchange-1.7, Sentence: BERT models have already been trained with various historical data sets and languages, including at least English, German, French, Latin and classical Chinese (Ehrmann et al., 2021; Yu and Wang, 2020; Labusch et al., 2019; Bamman and Burns, 2020) ., Distance: 0.3384034037590027\n",
      "Query: What is the optimal size of character n-grams for training a French FastText model?\n",
      "Document ID: W17-1223, Sentence: Character n-grams from length 2 to 7 are taken into account., Distance: 0.3425009846687317\n",
      "Document ID: 2020.findings-emnlp.138, Sentence: fastText includes character n-grams, allowing an assessment of the utility of subword information., Distance: 0.3483072519302368\n",
      "Query: Did anyone ever explore corrupting BPE tokenization at training time to improve test time generalization?\n",
      "Document ID: 2020.findings-emnlp.414, Sentence: (2018) performed further experiments to investigate the effects of tokenization on neural machine translation, but used a shared BPE vocabulary across all experiments., Distance: 0.3979998826980591\n",
      "Document ID: W19-6624, Sentence: We filter out training pairs where the source sentence was longer that 50 tokens (before applying BPE)., Distance: 0.41301727294921875\n",
      "Query: When were the synonym relationships found in biomedical ontologies used to train an entity linking model for the first time?\n",
      "Document ID: 2022.findings-emnlp.61, Sentence: Recently, a flurry of methods have been proposed to conduct biomedical entity representation learning from synonyms in the ontology, such as BIOSYN (Sung et al., 2020) , SapBERT (Liu et al., 2021) , and others (Lai et al., 2021) ., Distance: 0.253277063369751\n",
      "Document ID: P18-1010, Sentence: 2 New Corpora and Ontologies MedMentions Over the years researchers have constructed many large knowledge bases in the biomedical domain (Apweiler et al., 2004; Davis et al., 2008; Chatraryamontri et al., 2017) ., Distance: 0.3020244836807251\n",
      "Query: What method was developed to perform zero-shot entity linking for the biomedical domain in the absence of definitions for all concepts?\n",
      "Document ID: 2022.findings-emnlp.61, Sentence: Zero-shot entity linking has emerged as a promising direction for generalizing to new entities, but it still requires example gold entity mentions during training and canonical descriptions for all entities, both of which are rarely available outside of Wikipedia., Distance: 0.2731999158859253\n",
      "Document ID: 2022.findings-emnlp.61, Sentence: Recently, zeroshot entity linking has emerged as a promising direction for generalizing to unseen entities (Logeswaran et al., 2019; Wu et al., 2020) , by learning to encode contextual mentions for similarity comparison against reference entity descriptions., Distance: 0.3045438528060913\n",
      "Query: Which dataset could one use to train models to predict whether certain statements are entailed or contradicted by the patient history?\n",
      "Document ID: W19-5005, Sentence: In addition to showing which sentences in a document are most informative about the patient's condition, this method reveals the types of sentences that lead the model to make incorrect diagnoses., Distance: 0.3814302682876587\n",
      "Document ID: W19-5005, Sentence: In addition to showing which sentences in a document are most informative about the patient's condition, this method reveals the types of sentences that lead the model to make incorrect diagnoses., Distance: 0.3814302682876587\n",
      "Query: What technique did Nils Reimers and Iryna Gurevych use to train multilingual semantic models in 2020?\n",
      "Document ID: 2020.findings-emnlp.389, Sentence: A closer look at the {NLP} pipeline in monolingual and multilingual models., Distance: 0.3012605905532837\n",
      "Document ID: 2023.acl-long.693, Sentence: The data and models are available at https:// github.com/AI4Bharat/IndicBERT.. Introduction Recent advances in Natural Language Understanding are largely driven by pretrained multilingual models (Conneau et al., 2020; Xue et al., 2021; Doddapaneni et al., 2021) ., Distance: 0.3213461637496948\n",
      "Query: How can someone detect whether a sentence contains an idiomatic expression, or an expression used figuratively?\n",
      "Document ID: 2021.tacl-1.92, Sentence: To address this challenge, we study the task of detecting whether a sentence has an idiomatic expression and localizing it when it occurs in a figurative sense., Distance: 0.16835975646972656\n",
      "Document ID: 2021.tacl-1.92, Sentence: Idiomatic expressions are an integral part of natural language and constantly being added to a language., Distance: 0.3042016625404358\n",
      "KNN Metrics - Macro Precision: 0.228125, Macro Recall: 0.19270833333333331, Macro F1-Score: 0.19968750000000002, Micro Precision: 0.336734693877551, Micro Recall: 0.336734693877551, Micro F1-Score: 0.336734693877551, mAP: 0.336734693877551\n",
      "FAISS Results:\n",
      "Query: Which versions of the Morfessor tokenizer have been proposed in the literature?\n",
      "Document ID: 2020.lrec-1.486, Sentence: While Morfessor 2.0 only implements force splitting certain characters to single-character morphs, i.e., Distance: 0.7057371139526367\n",
      "Document ID: E14-2006, Sentence: A number of Morfessor variants have been developed later, including Morfessor Categories-MAP (Creutz and Lagus, 2005a) and Allomorfessor (Virpioja et al., 2010) ., Distance: 0.7211527824401855\n",
      "Query: What is Dynamic Programming Encoding?\n",
      "Document ID: P97-1038, Sentence: As Chen (1993) points out, dynamic programming is particularly susceptible to deletions occurring in one of the two languages., Distance: 0.8261194229125977\n",
      "Document ID: 2020.cl-2.5, Sentence: This analysis provides important insights about the process of encoding and its applicability in other tasks., Distance: 0.8414134383201599\n",
      "Query: How many variants of the byte-pair encoding subword tokenizer have there been proposed?\n",
      "Document ID: 2022.tacl-1.5, Sentence: Related Work Improvements to Subword Tokenization Further improvements to standard subword tokenization like Byte Pair Encoding (BPE) (Sennrich et al., 2016) , WordPiece (Wu et al., 2016) , and SentencePiece (Kudo and Richardson, 2018) have been proposed., Distance: 0.3872688412666321\n",
      "Document ID: 2021.emnlp-demo.4, Sentence: For transformer models, Byte-Pair Encoding (BPE) (Sennrich et al.,  2016) and SentencePiece (Kudo and Richardson,  2018) are popular subword tokenizers., Distance: 0.4274005889892578\n",
      "Query: Can I do topic modelling with embeddings?\n",
      "Document ID: P16-1063, Sentence: In our model, topics are represented by embedding vectors, and are shared across documents., Distance: 0.46789705753326416\n",
      "Document ID: P16-1063, Sentence: In our model, topics are represented by embedding vectors, and are shared across documents., Distance: 0.46789705753326416\n",
      "Query: Which paper invented additive attention?\n",
      "Document ID: D17-1151, Sentence: Attention Mechanism The two most commonly used attention mechanisms are the additive (Bahdanau et al., 2015) variant, equation ( 6 ) below, and the computationally less expensive multiplicative variant (Luong et al., 2015a) , equation ( 7 ) below., Distance: 0.7606053352355957\n",
      "Document ID: 2023.ranlp-1.121, Sentence: We used Loung attention mechanism., Distance: 0.863683819770813\n",
      "Query: Which variants of BERT (Bidirectional Encoding Representations from Transformers) are known?\n",
      "Document ID: 2022.naacl-main.156, Sentence: BERT (Bidirectional Encoder Representations from Transformers)., Distance: 0.17780253291130066\n",
      "Document ID: 2021.ranlp-1.147, Sentence: Recently the Bidirectional Encoder Representations from Transformers (BERT) model and its variants have gained enormous popularity due to its pretraining capability (Devlin et al., 2018) ., Distance: 0.2830314636230469\n",
      "Query: Of what concept are the tokenizers byte-pair encoding dropout (BPE-dropout) and unigram language model (ULM) an example?\n",
      "Document ID: 2020.acl-main.170, Sentence: In contrast to BPE, nearest neighbours of a token in the embedding space of BPE-dropout are often tokens that share sequences of characters with the original token., Distance: 0.6671578884124756\n",
      "Document ID: P18-1007, Sentence: BPE and the unigram language model share the same idea that they encode a text using fewer bits with a certain data compression principle (dictionary vs. entropy)., Distance: 0.6850683093070984\n",
      "Query: Which subword tokenizers can be trained with semi-supervised data?\n",
      "Document ID: 2021.emnlp-demo.4, Sentence: We use publicly available implementations of both subword tokenizers., Distance: 0.43872660398483276\n",
      "Document ID: 2022.tacl-1.5, Sentence: While recent tokenization approaches based on data-derived subword lexicons are less brittle than manually engineered tokenizers, these techniques are not equally suited to all languages, and the use of any fixed vocabulary may limit a model's ability to adapt., Distance: 0.5065029859542847\n",
      "Query: What is an open vocabulary?\n",
      "Document ID: W17-4706, Sentence: The vocabulary is more open in that respect., Distance: 0.5579173564910889\n",
      "Document ID: W18-1207, Sentence: But in reality, the vocabulary of a natural language is open., Distance: 0.5886127948760986\n",
      "Query: What word categories have a potentially transparent translation, such that a segmentation into appropriate subword units is sufficient to learn transparent translations?\n",
      "Document ID: P16-1162, Sentence: Our hypothesis is that a segmentation of rare words into appropriate subword units is sufficient to allow for the neural translation network to learn transparent translations, and to generalize this knowledge to translate and produce unseen words., Distance: 0.4892575144767761\n",
      "Document ID: P16-1162, Sentence: Word categories whose translation is potentially transparent include: • named entities., Distance: 0.5720019340515137\n",
      "Query: Who invented the WordPiece tokenizer?\n",
      "Document ID: 2020.findings-emnlp.138, Sentence: • WordPiece., Distance: 0.6925262808799744\n",
      "Document ID: 2021.wanlp-1.20, Sentence: The same wordpiece vocabulary from ARABERTv0.2 was used for tokenization., Distance: 0.74778813123703\n",
      "Query: What is vocabulary clustering?\n",
      "Document ID: 2020.lrec-1.612, Sentence: We use it to cluster the word embbeddings of the dataset vocabulary., Distance: 0.5121052265167236\n",
      "Document ID: R09-1075, Sentence: Clustering is an effect of the semantic association., Distance: 0.5288779735565186\n",
      "Query: Is it a good idea to compare BLEU scores across papers?\n",
      "Document ID: W18-6319, Sentence: Together, these issues make it difficult to evaluate and compare BLEU scores across papers, which impedes comparison and replication., Distance: 0.3224802613258362\n",
      "Document ID: W18-6319, Sentence: These parameters are often not reported or are hard to find, and consequently, BLEU scores between papers cannot be directly compared., Distance: 0.3744753301143646\n",
      "Query: Can you describe the phenomenon of non-concatenative morphology?\n",
      "Document ID: 2021.findings-emnlp.60, Sentence: Non-Concatenative Morphology The most interesting results can be seen for the two non-concatenative morphological phenomena: vowel harmony and reduplication., Distance: 0.3158484399318695\n",
      "Document ID: 2021.findings-emnlp.60, Sentence: However, our results do show a clear gap between the models' competence for non-concatenative and concatenative morphology., Distance: 0.42420873045921326\n",
      "Query: Is there an optimal number of BPE merge operations for transformers?\n",
      "Document ID: W19-6620, Sentence: Based on the findings, we make the following recommendations for selecting BPE merge operations in the future: • For Transformer-based architectures, we recommend the sweep be concentrated in the 0 − 4k range., Distance: 0.40198493003845215\n",
      "Document ID: W19-6620, Sentence: For our main experiments, we can already see a pretty consistent trend that for deep-transformer architecture, 0.5k and 32k merge operations always roughly correspond to the best and worst BPE configurations, respectively., Distance: 0.49016863107681274\n",
      "Query: What do we know about the anisotropy of contextualised embeddings?\n",
      "Document ID: D19-1006, Sentence: Given that isotropy has both theoretical and empirical benefits for static embeddings (Mu et al., 2018) , the extent of anisotropy in contextualized represen-tations is surprising., Distance: 0.40769195556640625\n",
      "Document ID: D19-1006, Sentence: We ultimately found that after adjusting for anisotropy, on average, less than 5% of the variance in a word's contextualized representations could be explained by a static embedding., Distance: 0.449637770652771\n",
      "Query: What are common features between S-BERT, SpanBERT and PhraseBERT?\n",
      "Document ID: 2020.coling-main.609, Sentence: 5.4 BERT vs. CharacterBERT: How Significant Is the Difference?, Distance: 0.4807402491569519\n",
      "Document ID: 2020.coling-main.609, Sentence: Figure 5 shows that CharacterBERT often improves over a vanilla BERT., Distance: 0.5494903326034546\n",
      "Query: Can you build a tokenizer using the theory of optimal transport?\n",
      "Document ID: 2023.emnlp-main.614, Sentence: Tokenization is an active area of research and various solutions based on data balancing (Johnson et al., 2017; Conneau and Lample, 2019) , optimal transport (Xu et al., 2021) , fuzzy subwords (Provilkov et al., 2020) , and many more (Chung et al., 2020; Tay et al., 2022) have been proposed., Distance: 0.6576724648475647\n",
      "Document ID: 2021.acl-long.571, Sentence: Intuitively, optimal transport is about finding the best transporting mass from the char distribution to the target token distribution with the minimum work defined by P , D ., Distance: 0.6693441867828369\n",
      "Query: Which character-based transformers have been proposed?\n",
      "Document ID: 2022.findings-acl.194, Sentence: Character-level modeling with Transformers appears to be more difficult., Distance: 0.49989864230155945\n",
      "Document ID: 2022.semeval-1.122, Sentence: 2018) for Transformers., Distance: 0.6848123073577881\n",
      "Query: What is the difference between ULM (Unigram LM) and SentencePiece?\n",
      "Document ID: W19-5005, Sentence: Somewhat surprisingly, the model trained on sentence embeddings outperformed the fine-tuned ULMFiT., Distance: 0.8139666318893433\n",
      "Document ID: 2020.findings-emnlp.414, Sentence: Since the unigram LM method selects tokens during vocabulary construction using a global optimization procedure, it does not produce junk tokens; this property also allows it to avoid merging frequent tokens with their neighbors too aggressively., Distance: 0.8359063863754272\n",
      "Query: Do we have systems that automatically split compounds into their constituents?\n",
      "Document ID: E03-1076, Sentence: Related Work While the linguistic properties of compounds are widely studied [Langer, 1998] , there has been only limited work on empirical methods to split up compounds for specific applications., Distance: 0.7067253589630127\n",
      "Document ID: E03-1076, Sentence: Splitting Options Compounds are created by joining existing words together., Distance: 0.745171070098877\n",
      "Query: Which papers introduced transition-based dependency parsing?\n",
      "Document ID: J11-1007, Sentence: Of particular note is past work on combining graph-based and transition-based dependency parsers., Distance: 0.3010450303554535\n",
      "Document ID: C12-1059, Sentence: Introduction The basic idea in transition-based dependency parsing is to define a nondeterministic transition system for mapping sentences to dependency trees and to perform parsing as search for the optimal transition sequence for a given sentence (Nivre, 2008) ., Distance: 0.3189256191253662\n",
      "Query: Which papers introduced graph-based dependency parsing?\n",
      "Document ID: J11-1007, Sentence: Of particular note is past work on combining graph-based and transition-based dependency parsers., Distance: 0.2749595046043396\n",
      "Document ID: K18-2022, Sentence: We propose a dependency parsing model based on the graph-based parser by Dozat and Manning (2016) ., Distance: 0.3459342122077942\n",
      "Query: Has there been an analysis comparing classic transition-based and graph-based dependency parsers?\n",
      "Document ID: J11-1007, Sentence: Of particular note is past work on combining graph-based and transition-based dependency parsers., Distance: 0.145639106631279\n",
      "Document ID: J11-1007, Sentence: As already noted, there are several recent developments in data-driven dependency parsing, which can be seen as targeting the specific weaknesses of traditional graphbased and transition-based models, respectively., Distance: 0.2987455129623413\n",
      "Query: What are some benchmarks that can measure the syntactic knowledge of a language model?\n",
      "Document ID: 2020.acl-demos.10, Sentence: Unlike broad-coverage language modeling metrics such as perplexity, these evaluations are targeted to reveal whether models have learned specific knowledge about the syntactic structure of language (see e.g., Distance: 0.5673941373825073\n",
      "Document ID: D18-1151, Sentence: We hope that our data set, and future extensions to other phenomena and languages, will make it possible to measure progress in syntactic language modeling and will lead to better understanding of the syntactic generalizations captured by language models., Distance: 0.575900137424469\n",
      "Query: What is a common architecture for neural dependency parsing?\n",
      "Document ID: N18-1037, Sentence: We start by describing our base dependency parsing model, which is neural network based and performs among the state-of-theart., Distance: 0.35561156272888184\n",
      "Document ID: D15-1158, Sentence: Recently, progress in dependency parsing has been made by introducing non-linear, neuralnetwork based models (Pei et al., 2015; Chen and Manning, 2014; Weiss et al., 2015; Dyer et al., 2015; Zhou et al., 2015) ., Distance: 0.39284664392471313\n",
      "Query: What system first incorporated adapters for cross-lingual dependency parsing?\n",
      "Document ID: 2020.emnlp-main.180, Sentence: Cross-Lingual Dependency Parsing The availability of consistent dependency treebanks in many languages (McDonald et al., 2013; Nivre et al., 2018) has provided an opportunity for the study of cross-lingual parsing., Distance: 0.404739111661911\n",
      "Document ID: 2021.acl-long.560, Sentence: Cross-lingual dependency parsing The new language., Distance: 0.41580092906951904\n",
      "Query: How can I probe for dependency structure in a model's representations?\n",
      "Document ID: W02-0603, Sentence: However, there is still room for considerable improvement in the model structure, especially regarding the representation of contextual dependencies., Distance: 0.6661404967308044\n",
      "Document ID: P04-1085, Sentence: contextual dependencies discussed in the previous subsection, we must augment the structure of our model to obtain a more general one., Distance: 0.738073468208313\n",
      "Query: What is an alternative to UD for dependency annotation?\n",
      "Document ID: W18-6008, Sentence: Introduction Universal Dependencies (UD) is an astonishing collaborative project of dozens of research groups around the world, developing an annotation scheme that is applicable to all languages and proposing treebanks based on that scheme for more than 70 languages from different language families (Nivre et al., Distance: 0.4329480230808258\n",
      "Document ID: 2023.eacl-main.76, Sentence: Universal Dependencies Universal Dependencies (UD) (Nivre et al., 2016 (Nivre et al., , 2020;; de Marneffe et al., 2021 ) is an initiative focused on the development of dependency treebanks., Distance: 0.44404900074005127\n",
      "Query: What are some common issues associated with the NLI task?\n",
      "Document ID: U14-1020, Sentence: An overview of the shared task results and a review of prior NLI work can be found in Tetreault et al., Distance: 0.46619975566864014\n",
      "Document ID: U14-1020, Sentence: Background NLI is a fairly recent, but rapidly growing area of research., Distance: 0.5103122591972351\n",
      "Query: Where can I find a discussion of the probing paradigm, including its shortcomings and disadvantages?\n",
      "Document ID: 2020.findings-emnlp.389, Sentence: Moreover, it would be interesting to investigate alternative probing strategies in order to better disentangle what pertains to the model itself from what is specific to a given probing strategy., Distance: 0.8060397505760193\n",
      "Document ID: 2020.lrec-1.130, Sentence: (4) Probes/Challenges: turns that questioned or disagreed with a prior idea., Distance: 0.8547036647796631\n",
      "Query: What is the \"attention is not explanation\" debate?\n",
      "Document ID: D19-1002, Sentence: A recent paper claims that 'Attention is not Explanation' (Jain and Wallace, 2019)., Distance: 0.29102790355682373\n",
      "Document ID: D19-1002, Sentence: A recent paper claims that 'Attention is not Explanation' (Jain and Wallace, 2019)., Distance: 0.29102790355682373\n",
      "Query: What is \"faithfulness\" in the context of interpretability?\n",
      "Document ID: 2020.acl-main.386, Sentence: Defining Faithfulness What does it mean for an interpretation method to be faithful?, Distance: 0.24765223264694214\n",
      "Document ID: 2020.acl-main.386, Sentence: To our knowledge, the term \"faithful interpretability\" was coined in Harrington et al., Distance: 0.35979539155960083\n",
      "Query: What are some studies that compare saliency methods in NLP?\n",
      "Document ID: 2021.naacl-main.399, Sentence: To address this question, we conduct a comprehensive and quantitative evaluation of saliency methods on a fundamental category of NLP models: neural language models., Distance: 0.4049828052520752\n",
      "Document ID: 2021.naacl-main.399, Sentence: To address this question, we conduct a comprehensive and quantitative evaluation of saliency methods on a fundamental category of NLP models: neural language models., Distance: 0.4049828052520752\n",
      "Query: What is a recurrent neural network grammar?\n",
      "Document ID: N16-1024, Sentence: Recurrent Neural Network Grammars., Distance: 0.3646817207336426\n",
      "Document ID: 2021.naacl-main.132, Sentence: 2019 ) is a recurrent neural network that explicitly models recursive structure through memory writing and erasing operations., Distance: 0.48927703499794006\n",
      "Query: What tasks comprise the GLUE suite?\n",
      "Document ID: 2020.acl-main.329, Sentence: With these in mind, our choice of tasks and datasets for GLUE-CoS are based on the following principles : • We choose a variety of tasks, ranging from simpler ones, on which the research community has already achieved high accuracies, to relatively more complex, on which very few attempts have been made., Distance: 0.5933074951171875\n",
      "Document ID: 2021.acl-long.260, Sentence: We leave the training details for downstream tasks and experiments on GLUE benchmark (Wang et al., 2018) in the appendix., Distance: 0.6243031024932861\n",
      "Query: What is item response theory? Has it been applied to NLP?\n",
      "Document ID: 2021.acl-long.92, Sentence: 2 Item Response Theory Baker and Kim (1993) introduce Item Response Theory (IRT), a statistical framework to measure the probability of a responder (human or AI system) predicting a correct answer for a given item (test example)., Distance: 0.4737139344215393\n",
      "Document ID: 2021.acl-long.346, Sentence: Item Response Theory (Lord et al., 1968; Baker, 2001 , IRT, reviewed in §2), a widely used (van Rijn et al., 2016) alternative in educational testing to simple summary statistics (Edgeworth, 1888) ., Distance: 0.5259488821029663\n",
      "Query: Why is the Japanese GSD treebank so easy to parse?\n",
      "Document ID: 2023.eacl-main.76, Sentence: Interestingly, the second and third-highest CONF treebanks are both Japanese: GSDLUW (0.93, 0.07, 130,298) and GSD (0.93, 0.07, 168, 333) ., Distance: 0.5904902219772339\n",
      "Document ID: W06-2920, Sentence: Another reason are the various annotation schemes and logical data formats used by different treebanks, which make it tedious to apply a parser to many treebanks., Distance: 0.6194434762001038\n",
      "Query: What is Targeted Syntactic Evaluation?\n",
      "Document ID: D18-1151, Sentence: Targeted Syntactic Evaluation of Language Models., Distance: 0.45242202281951904\n",
      "Document ID: 2020.acl-demos.10, Sentence: The targeted syntactic evaluation paradigm (Linzen et al., 2016; Lau et al., 2017; Gulordava et al., 2018; Marvin and Linzen, 2018; Futrell et al., 2019; Warstadt et al., 2020) incorporates methods from psycholinguistic experiments, designing sentences which hold most lexical and syntactic features of each sentence constant while minimally varying features that determine grammaticality or surprise characteristics of the sentence., Distance: 0.5065521597862244\n",
      "Query: What are negative polarity items?\n",
      "Document ID: 2020.tacl-1.54, Sentence: A study of negative polarity items (NPIs) by Warstadt et al., Distance: 0.3250364065170288\n",
      "Document ID: D18-1151, Sentence: Negative polarity items Negative polarity items, introduced in example (2) above, are words that (to a first approximation) need to occur in the context of negation., Distance: 0.4028584957122803\n",
      "Query: Are NLP models like BERT sensitive to word order? \n",
      "Document ID: 2021.emnlp-main.230, Sentence: Ettinger (2020) recently observed that BERT accuracy decreases for some word order perturbed examples, but not for others., Distance: 0.5081342458724976\n",
      "Document ID: P19-1334, Sentence: Its performance particularly stood out for the lexical overlap cases, suggesting that some of BERT's success at MNLI may be due to a greater tendency to incorporate word order information compared to other models., Distance: 0.5463711023330688\n",
      "Query: What are common syntactic annotation schemes?\n",
      "Document ID: P13-1023, Sentence: Syntactic annotation schemes come in many forms, from lexical categories such as POS tags to intricate hierarchical structures., Distance: 0.35867154598236084\n",
      "Document ID: P13-1023, Sentence: We view this as a major advantage of semantic annotation schemes over their syntactic counterparts, especially given the huge amount of manual labor required for large syntactic annotation projects., Distance: 0.41650426387786865\n",
      "Query: What are common semantic annotation schemes?\n",
      "Document ID: W05-1714, Sentence: • What types of annotation schemes and formats are applied?, Distance: 0.4412769675254822\n",
      "Document ID: W14-5149, Sentence: Many such annotations schemas have been proposed for different NLP applications., Distance: 0.4826483130455017\n",
      "Query: What are universals?\n",
      "Document ID: 2021.cl-2.11, Sentence: Universal Dependencies., Distance: 0.7706007957458496\n",
      "Document ID: J19-3005, Sentence: Using such universals, features can be deduced by modus ponens., Distance: 0.8278723955154419\n",
      "Query: Which databased can I use to analyse typological information?\n",
      "Document ID: J19-3005, Sentence: In fact, typological database documentation is incomplete, approximate, and discrete., Distance: 0.617300271987915\n",
      "Document ID: J19-3005, Sentence: (ii) What are the advantages and limitations of currently available typological databases?, Distance: 0.6378562450408936\n",
      "Query: What is the purpose of language embeddings?\n",
      "Document ID: 2021.acl-long.560, Sentence: language embeddings., Distance: 0.46421849727630615\n",
      "Document ID: 2023.nodalida-1.67, Sentence: Further analysis has shown that the nature of the underlying data used to generate language embeddings can have a significant impact on what features are encapsulated (Bjerva and Augenstein, 2018a,b; Bjerva et al., 2019b) , and even that such representations contain typological generalisations ( Östling and Kurfalı, 2023) ., Distance: 0.5212963223457336\n",
      "Query: What technique can be used to try to explain the inner workings of a language model?\n",
      "Document ID: W18-5424, Sentence: Do Language Models Understand Anything?, Distance: 0.5977280139923096\n",
      "Document ID: 2020.emnlp-main.263, Sentence: There also exist explainability methods that generate textual explanations (Camburu et al., 2018) and are trained posthoc or jointly with the model at hand., Distance: 0.7026523351669312\n",
      "Query: What is weak supervision?\n",
      "Document ID: 2023.acl-long.796, Sentence: Related work Weak supervision., Distance: 0.3392060399055481\n",
      "Document ID: 2023.acl-long.796, Sentence: However, in a broader context, weak supervision can have different forms., Distance: 0.34442442655563354\n",
      "Query: What is the name of the first Dutch transformer-based pre-trained language model?\n",
      "Document ID: 2020.findings-emnlp.292, Sentence: The transformer model led to other popular language models, e.g., Distance: 0.5050188302993774\n",
      "Document ID: 2020.emnlp-main.263, Sentence: For the Transformer model, we fine-tune the pre-trained basic, uncased language model (LM) (Wolf et al., 2019) ., Distance: 0.5112684965133667\n",
      "Query: What is BERTology?\n",
      "Document ID: 2020.tacl-1.54, Sentence: A Primer in BERTology: What We Know About How BERT Works., Distance: 0.48606008291244507\n",
      "Document ID: 2021.eacl-main.270, Sentence: Syntactic BERTology., Distance: 0.5458081960678101\n",
      "Query: What is the name of the research initiative creating resources for African languages?\n",
      "Document ID: 2023.acl-long.609, Sentence: First, it is the second most available resource after the religious domain for most African languages., Distance: 0.6071586608886719\n",
      "Document ID: 2021.gem-1.10, Sentence: Chris Emezue and Rubungo Andre Niyongabo explored potential low-resource African languages for crowdsourcing., Distance: 0.6607020497322083\n",
      "Query: What is the WMT conference about?\n",
      "Document ID: 2023.wmt-1.0, Sentence: This is the eighth time WMT has been held as a conference., Distance: 0.41752004623413086\n",
      "Document ID: 2023.wmt-1.0, Sentence: This is the eighth time WMT has been held as a conference., Distance: 0.41752004623413086\n",
      "Query: How many tasks did SemEval 2022 have?\n",
      "Document ID: 2022.semeval-1.0, Sentence: The tasks for SemEval-2022 were proposed in 2021, and next year's tasks have already been selected and are underway.., Distance: 0.4491610527038574\n",
      "Document ID: 2022.semeval-1.0, Sentence: Introduction Welcome to SemEval-2022!, Distance: 0.7196040153503418\n",
      "Query: What is the modular and parameter-efficient alternative to fine-tuning called?\n",
      "Document ID: 2023.acl-long.796, Sentence: AdapterHub (Pfeiffer et al., 2020) is used for implementing parameter-efficient fine-tuning., Distance: 0.44974541664123535\n",
      "Document ID: 2023.emnlp-demo.13, Sentence: First, parameter-efficient fine-tuning (Lialin et al., 2023; Sabry and Belz, 2023) focuses on the aspect of computational efficiency and feasibility by only fine-tuning a small * Authors contributed equally., Distance: 0.4835522770881653\n",
      "Query: In what task does post-editing play a role?\n",
      "Document ID: 2022.emnlp-main.532, Sentence: Post-editing., Distance: 0.5272001624107361\n",
      "Document ID: 2013.mtsummit-wptp.5, Sentence: Tasks The users performed the post-editing tasks using a portal that was especially developed for postediting, the interface of which is displayed in -For example, try to rectify word order and spelling when they are inappropriate to the extent that the text has become impossible or difficult to comprehend., Distance: 0.6031880974769592\n",
      "Query: Are certain researchers critical of parsing?\n",
      "Document ID: J11-1007, Sentence: Other studies have tried to overcome the weaknesses of parsing models by changing the underlying model structure directly., Distance: 0.5898631811141968\n",
      "Document ID: W02-0905, Sentence: In terms of parsing development, it is broadly assumed that parsers need such information in order to reduce the number of possible analyses and, therefore, solve syntactic ambiguity., Distance: 0.5904872417449951\n",
      "Query: What measure has been proposed to stimulate research in multilingual NLP?\n",
      "Document ID: N19-1367, Sentence: More generally, multilingual NLP is an active and growing area of research., Distance: 0.2908543348312378\n",
      "Document ID: J19-3005, Sentence: In this section we discuss potential future research avenues that may result in a closer and more effective integration of linguistic typology and multilingual NLP., Distance: 0.3986959457397461\n",
      "Query: What dataset can I use to test multilingual information retrieval?\n",
      "Document ID: 2023.mrl-1.21, Sentence: Our dataset is of a scale to accommodate both large-scale training and evaluation of multilingual retrieval methods., Distance: 0.3945668637752533\n",
      "Document ID: 2023.mrl-1.21, Sentence: While progress towards multilingual retrieval through the release of datasets such as Mr. TYDI (Zhang et al., 2021) and mMARCO (Bonifacio et al., 2021) , both are limited in that they evaluate monolingual retrieval for a range of languages, rather than true multilingual retrieval, using multiple languages simultaneously., Distance: 0.43699416518211365\n",
      "Query: Is the impact of big tech on NLP growing?\n",
      "Document ID: 2023.acl-long.734, Sentence: Overall, the presence of Big Tech companies in NLP research has contributed to the growth and advancement of the field., Distance: 0.2934686541557312\n",
      "Document ID: 2023.acl-long.734, Sentence: In turn, we are observing a greater presence of large technology companies (Big Tech) on NLP research than ever before., Distance: 0.3659141957759857\n",
      "Query: What are drawbacks of character-level machine translation?\n",
      "Document ID: 2022.findings-acl.194, Sentence: We speculate that the main reasons for not considering character-level modeling are its lower efficiency and the fact that the literature shows no clear improvement of translation quality., Distance: 0.4880269169807434\n",
      "Document ID: 2022.findings-acl.194, Sentence: We present a literature and empirical survey that critically assesses the state of the art in character-level modeling for machine translation (MT)., Distance: 0.540229856967926\n",
      "Query: What is the BabyLM Challenge?\n",
      "Document ID: 2023.conll-babylm.1, Sentence: The BabyLM Challenge is a shared task that invites * Equal contribution., Distance: 0.42374303936958313\n",
      "Document ID: 2023.conll-babylm.1, Sentence: (2023) Future BabyLM Challenges The first iteration of the BabyLM Challenge yielded many successes, but also some organizational and scientific challenges., Distance: 0.4889087378978729\n",
      "Query: What does preregistration mean in research?\n",
      "Document ID: 2021.naacl-main.51, Sentence: Preregistration refers to the practice of specifying what you are going to do, and what you expect to find in your study, before carrying out the study., Distance: 0.23372836410999298\n",
      "Document ID: 2021.naacl-main.51, Sentence: How does preregistration work?, Distance: 0.3955954909324646\n",
      "Query: How can a transition-based parser deal with non-projectivity?\n",
      "Document ID: N16-1024, Sentence: Constraints on parser transitions., Distance: 0.3280421495437622\n",
      "Document ID: Q15-1026, Sentence: Transition-based Parsing., Distance: 0.4042315185070038\n",
      "Query: How is the input represented in a delexicalized dependency parser?\n",
      "Document ID: P05-1013, Sentence: The parser builds dependency graphs by traversing the input from left to right, using a stack to store tokens that are not yet complete with respect to their dependents., Distance: 0.5290005207061768\n",
      "Document ID: D11-1006, Sentence: Multi-Source Transfer of Delexicalized Dependency Parsers., Distance: 0.5329874753952026\n",
      "Query: What are called parsing oracles that are non-deterministic?\n",
      "Document ID: Q13-1033, Sentence: Training Deterministic Parsers with Non-Deterministic Oracles., Distance: 0.37551048398017883\n",
      "Document ID: Q13-1033, Sentence: Dynamic oracles are non-deterministic in the sense that they return a set of valid transitions for a given parser state and gold tree., Distance: 0.3777853846549988\n",
      "Query: What are the four transitions of an arc-eager dependency parser?\n",
      "Document ID: J08-4003, Sentence: Arc-Eager Parsing The transition set T for the arc-eager, stack-based parser is defined in Figure 5 and contains four types of transitions: 1., Distance: 0.3615016043186188\n",
      "Document ID: J08-4003, Sentence: The arc-eager parser differs from the arc-standard one by attaching right dependents (using RIGHT-ARC e l transitions) as soon as possible, that is, before the right dependent has found all its right dependents., Distance: 0.4103921055793762\n",
      "Query: What languages did the CoNLL 2006 shared task include?\n",
      "Document ID: D07-1013, Sentence: on a variety of languages, as seen in Table 1 , which shows results for the two top performing systems in the CoNLL-X shared task, McDonald et al., Distance: 0.5954788327217102\n",
      "Document ID: C16-1147, Sentence: Some of these tasks featured languages other than English., Distance: 0.642387866973877\n",
      "Query: What was the input of the CoNLL 2017 shared task?\n",
      "Document ID: K17-3001v2, Sentence: CoNLL 2017 has picked up the threads of those pioneering tasks and addressed these two issues., Distance: 0.4967290163040161\n",
      "Document ID: E17-1027, Sentence: We use the version of the data provided to the CoNLL 2016 Shared Task participants., Distance: 0.5970456600189209\n",
      "Query: What dataset can be used for multilingual natural language inference?\n",
      "Document ID: 2021.americasnlp-1.23, Sentence: For development and testing, we used parallel sentences belonging to a new natural language inference dataset for the 10 indigenous languages featured in our shared task, which is a manual translation of the Spanish version of the multilingual XNLI dataset (Conneau et al., 2018) ., Distance: 0.5209593772888184\n",
      "Document ID: 2022.naacl-main.125, Sentence: B Datasets We employ three datasets for text classification and three datasets for natural language inference as described below., Distance: 0.5274575352668762\n",
      "Query: What are visual dependency relations?\n",
      "Document ID: D13-1128, Sentence: Table 1 : Visual Dependency Grammar defines eight relations between pairs of annotated regions., Distance: 0.42083799839019775\n",
      "Document ID: D13-1128, Sentence: Visual Dependency Representation In analogy to dependency grammar for natural language syntax, we define Visual Dependency Grammar to describe the spatial relations between pairs of image regions., Distance: 0.49816378951072693\n",
      "Query: How can the digital status of languages be categorized?\n",
      "Document ID: 2020.acl-main.560, Sentence: The Six Kinds of Languages In order to summarize the digital status and 'richness' of languages in the context of data availability, we propose a taxonomy based on the number of language resources which exist for different languages., Distance: 0.6037042737007141\n",
      "Document ID: 2021.cl-2.11, Sentence: These categories are widely attested in the world's languages., Distance: 0.6828558444976807\n",
      "Query: What toolkit can be used to easily fine-tune contextualized embeddings in a multi-task setting?\n",
      "Document ID: 2021.eacl-demos.22, Sentence: In this paper we present MACHAMP, a toolkit for easy fine-tuning of contextualized embeddings in multi-task settings., Distance: 0.19080215692520142\n",
      "Document ID: 2021.eacl-demos.22, Sentence: In this paper we present MACHAMP, a toolkit for easy fine-tuning of contextualized embeddings in multi-task settings., Distance: 0.19080215692520142\n",
      "Query: What is the name of the statistical machine translation toolkit developed by Philip Koehn and colleagues?\n",
      "Document ID: D16-1162, Sentence: (2014) ) is a variant of statistical machine translation (SMT; Brown et al., Distance: 0.43162646889686584\n",
      "Document ID: P12-3008, Sentence: The Moses SMT toolkit (Koehn et al., 2007 ) provides a complete statistical translation system distributed under the LGPL license., Distance: 0.5362644195556641\n",
      "Query: What kind of encoder can be used to incorporate syntactic structure into a neural machine translation system?\n",
      "Document ID: D17-1209, Sentence: We present a simple and effective approach to incorporating syntactic structure into neural attention-based encoderdecoder models for machine translation., Distance: 0.5546935200691223\n",
      "Document ID: Q17-1004, Sentence: Inspired by the encoder-decoder framework for neural machine translation (Bahdanau et al., 2015; Cho et al., 2014) , we use an LSTM to capture full sentence features, and another LSTM to generate the constituent hierarchies for each word., Distance: 0.5909678339958191\n",
      "Query: Who developed ELMo?\n",
      "Document ID: 2021.eacl-main.179, Sentence: ELMo., Distance: 0.5450807809829712\n",
      "Document ID: 2020.lrec-1.223, Sentence: • ELMo., Distance: 0.6137440800666809\n",
      "Query: What is a diagnostic classifier?\n",
      "Document ID: 2020.acl-main.682, Sentence: The diagnostic classifier outputs d a logits where each of them models the probability P (y k = 1|d) (where d is dialogue state representation), one for each attribute y k to be predicted., Distance: 0.5992516279220581\n",
      "Document ID: 2020.cl-2.4, Sentence: (2018a) , we use diagnostic classifiers (Shi, Padhi, and Knight 2016; Adi et al., Distance: 0.6281558275222778\n",
      "Query: Can LSTMs learn syntactic dependencies?\n",
      "Document ID: Q16-1037, Sentence: Assessing the Ability of {LSTM}s to Learn Syntax-Sensitive Dependencies., Distance: 0.34022486209869385\n",
      "Document ID: Q16-1037, Sentence: Linguistic regularities are often sensitive to syntactic structure; can such dependencies be captured by LSTMs, which do not have explicit structural representations?, Distance: 0.36409953236579895\n",
      "Query: Which teams participated in the Americas 2021 shared task?\n",
      "Document ID: 2021.americasnlp-1.23, Sentence: Eight teams participated in the AmericasNLP 2021 Shared Task on OMT., Distance: 0.5496017932891846\n",
      "Document ID: W18-2409, Sentence: (Eq.9) 4 Participation in the Shared Task A total of six teams from eight different institutions participated in the NEWS 2018 Shared Task., Distance: 0.7293365001678467\n",
      "Query: Which team ranked first in the CoNLL 2018 shared task based on the LAS score\n",
      "Document ID: 2020.emnlp-main.180, Sentence: For evaluation, the official CoNLL 2018 Shared Task script 7 is used to obtain LAS scores on the test set of each treebank., Distance: 0.6961662769317627\n",
      "Document ID: J08-4003, Sentence: For comparison, we also include the results of the two top scoring systems in the CoNLL-X shared task, those of McDonald, Lerman, and Pereira (2006) and Nivre et al., Distance: 0.7116425037384033\n",
      "Query: Who developed Udify?\n",
      "Document ID: D19-1279, Sentence: Code for UDify is available at https:// github.com/hyperparticle/udify., Distance: 0.7581583261489868\n",
      "Document ID: D19-1279, Sentence: com/hyperparticle/udify., Distance: 0.795513927936554\n",
      "Query: How can masked language models be evaluated intrinsically?\n",
      "Document ID: 2023.conll-babylm.1, Sentence: (2023) simplified the masked language modeling objective by coarse-graining the output classes, with little effect., Distance: 0.448461651802063\n",
      "Document ID: 2020.acl-main.240, Sentence: Masked Language Model Scoring., Distance: 0.4816322326660156\n",
      "Query: What are some relevant benchmarks for code-mixed NLP?\n",
      "Document ID: 2021.gem-1.10, Sentence: Benchmarks in NLG In this section, we summarize common criticisms of benchmarks in NLP, discuss how they apply to NLG, and how we plan to address them., Distance: 0.5443246364593506\n",
      "Document ID: 2021.bppf-1.1, Sentence: In this talk, I will argue for three desired interrelated shifts in NLP benchmarking, which motivate and support each other, that should direct further research., Distance: 0.5802299976348877\n",
      "Query: What are some multilingual pretrained language models for Indic languages?\n",
      "Document ID: 2020.findings-emnlp.445, Sentence: For Indic languages, two such multilingual models are available: XLM-R (Conneau et al., 2020) and multilingual BERT (Devlin et al., 2019) ., Distance: 0.32298094034194946\n",
      "Document ID: 2023.acl-long.693, Sentence: Models Most multilingual pretrained language models and their variants like mBERT (Devlin et al., 2019) , mT5 (Xue et al., 2021) , and XLM (Conneau and Lample, 2019) are trained on major Indic languages., Distance: 0.3572426438331604\n",
      "Query: What are some multilingual pretrained models for African languages?\n",
      "Document ID: 2021.mrl-1.11, Sentence: We introduce AfriBERTa, a transformer-based multilingual language models trained on 11 African languages, all of which are low-resource., Distance: 0.39548811316490173\n",
      "Document ID: 2021.mrl-1.11, Sentence: Besides showing that multilingual language models are viable on low-resource African languages with small training data, we also introduce the first language models for four of these languages: Kinyarwanda, Kirundi, Nigerian Pidgin and Tigrinya., Distance: 0.41547438502311707\n",
      "Query: What are some datasets you can use to probe models for linguistic information? \n",
      "Document ID: 2023.findings-acl.185, Sentence: There are some datasets that come from speech transcription, news, dialogues, books, government documents, and treebanks., Distance: 0.6203614473342896\n",
      "Document ID: 2020.findings-emnlp.445, Sentence: Datasets are available for some tasks for a few languages., Distance: 0.6307176351547241\n",
      "Query: Can you transliterate languages for better language modelling?\n",
      "Document ID: 2023.findings-eacl.50, Sentence: Does Transliteration Help Multilingual Language Modeling?., Distance: 0.26730963587760925\n",
      "Document ID: 2023.findings-eacl.50, Sentence: Transliteration in Language Modeling Different works have applied transliteration in different aspect for language models., Distance: 0.38089004158973694\n",
      "Query: What are some surveys on approaches to code-switched NLP?\n",
      "Document ID: 2023.findings-acl.185, Sentence: We hope this survey can encourage and lead NLP researchers in a better direction on code-switching research., Distance: 0.33104002475738525\n",
      "Document ID: 2023.findings-acl.185, Sentence: The Decades Progress on Code-Switching Research in NLP: A Systematic Survey on Trends and Challenges., Distance: 0.384879469871521\n",
      "Query: What are some feature attribution metrics for language model interpretability \n",
      "Document ID: 2020.cl-2.4, Sentence: Interpretability studies have been one of the emerging trends in NLP as hinted by the on-going Representation Evaluation (RepEval) (Nangia et al., Distance: 0.6637780070304871\n",
      "Document ID: 2020.acl-main.386, Sentence: Explanations provided by \"inherently interpretable\" models must be held to the same standards as post-hoc interpretation methods, and be evaluated for faithfulness using the same set of evaluation techniques., Distance: 0.7163554430007935\n",
      "Query: What are natural language understanding benchmarks for Indic languages\n",
      "Document ID: 2020.findings-emnlp.445, Sentence: IndicGLUE: Multilingual NLU Benchmark We now introduce IndicGLUE, the Indic General Language Understanding Evaluation Benchmark, which is a collection of various NLP tasks as described below., Distance: 0.38006651401519775\n",
      "Document ID: 2020.findings-emnlp.445, Sentence: Such a benchmark is missing for Indic languages and the goal of this work is to fill this void., Distance: 0.38335418701171875\n",
      "Query: What is a toolkit for easy multi-task finetuning of models?\n",
      "Document ID: 2022.tacl-1.25, Sentence: Popular benchmarks usually aggregate multiple tasks to spur the progress of generalizable models., Distance: 0.7070397138595581\n",
      "Document ID: 2022.tacl-1.25, Sentence: The benchmarks usually aggregate multiple model-agnostic tasks under a unified framework, enabling researchers to fairly compare different models., Distance: 0.7132364511489868\n",
      "Query: How much data is required to adapt a BERT model to another language of the same family?\n",
      "Document ID: 2021.findings-acl.433, Sentence: In-stead, monolingual BERT models are transferable to languages with very little data if the source and target languages are relatively similar., Distance: 0.49751266837120056\n",
      "Document ID: 2022.lchange-1.7, Sentence: BERT models have already been trained with various historical data sets and languages, including at least English, German, French, Latin and classical Chinese (Ehrmann et al., 2021; Yu and Wang, 2020; Labusch et al., 2019; Bamman and Burns, 2020) ., Distance: 0.6768068075180054\n",
      "Query: What is the optimal size of character n-grams for training a French FastText model?\n",
      "Document ID: W17-1223, Sentence: Character n-grams from length 2 to 7 are taken into account., Distance: 0.6850019693374634\n",
      "Document ID: 2020.findings-emnlp.138, Sentence: fastText includes character n-grams, allowing an assessment of the utility of subword information., Distance: 0.6966146230697632\n",
      "Query: Did anyone ever explore corrupting BPE tokenization at training time to improve test time generalization?\n",
      "Document ID: 2020.findings-emnlp.414, Sentence: (2018) performed further experiments to investigate the effects of tokenization on neural machine translation, but used a shared BPE vocabulary across all experiments., Distance: 0.7959997653961182\n",
      "Document ID: W19-6624, Sentence: We filter out training pairs where the source sentence was longer that 50 tokens (before applying BPE)., Distance: 0.8260347843170166\n",
      "Query: When were the synonym relationships found in biomedical ontologies used to train an entity linking model for the first time?\n",
      "Document ID: 2022.findings-emnlp.61, Sentence: Recently, a flurry of methods have been proposed to conduct biomedical entity representation learning from synonyms in the ontology, such as BIOSYN (Sung et al., 2020) , SapBERT (Liu et al., 2021) , and others (Lai et al., 2021) ., Distance: 0.5065542459487915\n",
      "Document ID: P18-1010, Sentence: 2 New Corpora and Ontologies MedMentions Over the years researchers have constructed many large knowledge bases in the biomedical domain (Apweiler et al., 2004; Davis et al., 2008; Chatraryamontri et al., 2017) ., Distance: 0.6040489673614502\n",
      "Query: What method was developed to perform zero-shot entity linking for the biomedical domain in the absence of definitions for all concepts?\n",
      "Document ID: 2022.findings-emnlp.61, Sentence: Zero-shot entity linking has emerged as a promising direction for generalizing to new entities, but it still requires example gold entity mentions during training and canonical descriptions for all entities, both of which are rarely available outside of Wikipedia., Distance: 0.5463998913764954\n",
      "Document ID: 2022.findings-emnlp.61, Sentence: Recently, zeroshot entity linking has emerged as a promising direction for generalizing to unseen entities (Logeswaran et al., 2019; Wu et al., 2020) , by learning to encode contextual mentions for similarity comparison against reference entity descriptions., Distance: 0.6090877056121826\n",
      "Query: Which dataset could one use to train models to predict whether certain statements are entailed or contradicted by the patient history?\n",
      "Document ID: W19-5005, Sentence: In addition to showing which sentences in a document are most informative about the patient's condition, this method reveals the types of sentences that lead the model to make incorrect diagnoses., Distance: 0.7628605365753174\n",
      "Document ID: W19-5005, Sentence: In addition to showing which sentences in a document are most informative about the patient's condition, this method reveals the types of sentences that lead the model to make incorrect diagnoses., Distance: 0.7628605365753174\n",
      "Query: What technique did Nils Reimers and Iryna Gurevych use to train multilingual semantic models in 2020?\n",
      "Document ID: 2020.findings-emnlp.389, Sentence: A closer look at the {NLP} pipeline in monolingual and multilingual models., Distance: 0.6025212407112122\n",
      "Document ID: 2023.acl-long.693, Sentence: The data and models are available at https:// github.com/AI4Bharat/IndicBERT.. Introduction Recent advances in Natural Language Understanding are largely driven by pretrained multilingual models (Conneau et al., 2020; Xue et al., 2021; Doddapaneni et al., 2021) ., Distance: 0.6426924467086792\n",
      "Query: How can someone detect whether a sentence contains an idiomatic expression, or an expression used figuratively?\n",
      "Document ID: 2021.tacl-1.92, Sentence: To address this challenge, we study the task of detecting whether a sentence has an idiomatic expression and localizing it when it occurs in a figurative sense., Distance: 0.3367195725440979\n",
      "Document ID: 2021.tacl-1.92, Sentence: Idiomatic expressions are an integral part of natural language and constantly being added to a language., Distance: 0.6084035038948059\n",
      "FAISS Metrics - Macro Precision: 0.228125, Macro Recall: 0.19270833333333331, Macro F1-Score: 0.19968750000000002, Micro Precision: 0.336734693877551, Micro Recall: 0.336734693877551, Micro F1-Score: 0.336734693877551, mAP: 0.336734693877551\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "print(\"KNN Results:\")\n",
    "for query, indices, distances in zip(queries, knn_indices_list, knn_distances_list):\n",
    "    print(f\"Query: {query}\")\n",
    "    for idx, dist in zip(indices, distances):\n",
    "        print(f\"Document ID: {sentence_to_doc_map[idx]}, Sentence: {all_sentences[idx]}, Distance: {dist}\")\n",
    "\n",
    "print(f\"KNN Metrics - Macro Precision: {knn_metrics[0]}, Macro Recall: {knn_metrics[1]}, Macro F1-Score: {knn_metrics[2]}, Micro Precision: {knn_metrics[3]}, Micro Recall: {knn_metrics[4]}, Micro F1-Score: {knn_metrics[5]}, mAP: {knn_metrics[6]}\")\n",
    "\n",
    "print(\"FAISS Results:\")\n",
    "for query, indices, distances in zip(queries, faiss_indices_list, faiss_distances_list):\n",
    "    print(f\"Query: {query}\")\n",
    "    for idx, dist in zip(indices, distances):\n",
    "        print(f\"Document ID: {sentence_to_doc_map[idx]}, Sentence: {all_sentences[idx]}, Distance: {dist}\")\n",
    "\n",
    "print(f\"FAISS Metrics - Macro Precision: {faiss_metrics[0]}, Macro Recall: {faiss_metrics[1]}, Macro F1-Score: {faiss_metrics[2]}, Micro Precision: {faiss_metrics[3]}, Micro Recall: {faiss_metrics[4]}, Micro F1-Score: {faiss_metrics[5]}, mAP: {faiss_metrics[6]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "962a6fdc-f603-4f5a-9ab1-f798967a851d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS Metrics -\n",
      " Macro Precision: 0.228125,\n",
      " Macro Recall: 0.19270833333333331,\n",
      " Macro F1-Score: 0.19968750000000002,\n",
      " Micro Precision: 0.336734693877551,\n",
      " Micro Recall: 0.336734693877551,\n",
      " Micro F1-Score: 0.336734693877551,\n",
      " mAP: 0.336734693877551\n"
     ]
    }
   ],
   "source": [
    "print(f\"FAISS Metrics -\\n Macro Precision: {faiss_metrics[0]},\\n Macro Recall: {faiss_metrics[1]},\\n Macro F1-Score: {faiss_metrics[2]},\\n Micro Precision: {faiss_metrics[3]},\\n Micro Recall: {faiss_metrics[4]},\\n Micro F1-Score: {faiss_metrics[5]},\\n mAP: {faiss_metrics[6]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ae907cbc-14ee-4033-ae4c-4b70cc8141c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Metrics -\n",
      " Macro Precision: 0.228125,\n",
      " Macro Recall: 0.19270833333333331,\n",
      " Macro F1-Score: 0.19968750000000002,\n",
      " Micro Precision: 0.336734693877551,\n",
      " Micro Recall: 0.336734693877551,\n",
      " Micro F1-Score: 0.336734693877551,\n",
      " mAP: 0.336734693877551\n"
     ]
    }
   ],
   "source": [
    "print(f\"KNN Metrics -\\n Macro Precision: {knn_metrics[0]},\\n Macro Recall: {knn_metrics[1]},\\n Macro F1-Score: {knn_metrics[2]},\\n Micro Precision: {knn_metrics[3]},\\n Micro Recall: {knn_metrics[4]},\\n Micro F1-Score: {knn_metrics[5]},\\n mAP: {knn_metrics[6]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dcb2da-337c-45f0-83bb-afdbde333420",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ca5ccf46-856a-4973-bfd9-8b49cb023ee0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1277, 196]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m     map_score \u001b[38;5;241m=\u001b[39m average_precision_score(relevance, [\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(relevance))\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m precision, recall, f1, micro_precision, micro_recall, micro_f1, map_score\n\u001b[0;32m---> 18\u001b[0m knn_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mground_truth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mknn_predictions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m faiss_metrics \u001b[38;5;241m=\u001b[39m calculate_metrics(ground_truth, faiss_predictions)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Print results\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[44], line 9\u001b[0m, in \u001b[0;36mcalculate_metrics\u001b[0;34m(ground_truth, predictions)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Convert ground truth to binary relevance\u001b[39;00m\n\u001b[1;32m      7\u001b[0m relevance \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m gt \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m pred, gt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(flattened_predictions, ground_truth \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(predictions[\u001b[38;5;241m0\u001b[39m]))]\n\u001b[0;32m----> 9\u001b[0m precision \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflattened_ground_truth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflattened_predictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmacro\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m recall \u001b[38;5;241m=\u001b[39m recall_score(flattened_ground_truth, flattened_predictions, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m f1 \u001b[38;5;241m=\u001b[39m f1_score(flattened_ground_truth, flattened_predictions, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/virt/py310/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/virt/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:2182\u001b[0m, in \u001b[0;36mprecision_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   2015\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m   2016\u001b[0m     {\n\u001b[1;32m   2017\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2042\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2043\u001b[0m ):\n\u001b[1;32m   2044\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the precision.\u001b[39;00m\n\u001b[1;32m   2045\u001b[0m \n\u001b[1;32m   2046\u001b[0m \u001b[38;5;124;03m    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2180\u001b[0m \u001b[38;5;124;03m    array([0.5, 1. , 1. ])\u001b[39;00m\n\u001b[1;32m   2181\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2182\u001b[0m     p, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_recall_fscore_support\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2183\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2184\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2187\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarn_for\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprecision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2189\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2191\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m p\n",
      "File \u001b[0;32m~/virt/py310/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:186\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[0;32m~/virt/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1767\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1604\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute precision, recall, F-measure and support for each class.\u001b[39;00m\n\u001b[1;32m   1605\u001b[0m \n\u001b[1;32m   1606\u001b[0m \u001b[38;5;124;03mThe precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1764\u001b[0m \u001b[38;5;124;03m array([2, 2, 2]))\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m _check_zero_division(zero_division)\n\u001b[0;32m-> 1767\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43m_check_set_wise_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1769\u001b[0m \u001b[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[1;32m   1770\u001b[0m samplewise \u001b[38;5;241m=\u001b[39m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/virt/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1539\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m average \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m average_options \u001b[38;5;129;01mand\u001b[39;00m average \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1537\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage has to be one of \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(average_options))\n\u001b[0;32m-> 1539\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[38;5;66;03m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[39;00m\n\u001b[1;32m   1541\u001b[0m \u001b[38;5;66;03m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[39;00m\n\u001b[1;32m   1542\u001b[0m present_labels \u001b[38;5;241m=\u001b[39m unique_labels(y_true, y_pred)\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m~/virt/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:85\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[1;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m     \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m     type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     87\u001b[0m     type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/virt/py310/lib/python3.10/site-packages/sklearn/utils/validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    460\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1277, 196]"
     ]
    }
   ],
   "source": [
    "# Calculate metrics\n",
    "def calculate_metrics(ground_truth, predictions):\n",
    "    flattened_ground_truth = [gt for sublist in ground_truth for gt in sublist]\n",
    "    flattened_predictions = [pred for sublist in predictions for pred in sublist]\n",
    "    \n",
    "    # Convert ground truth to binary relevance\n",
    "    relevance = [1 if pred in gt else 0 for pred, gt in zip(flattened_predictions, ground_truth * len(predictions[0]))]\n",
    "    \n",
    "    precision = precision_score(flattened_ground_truth, flattened_predictions, average='macro')\n",
    "    recall = recall_score(flattened_ground_truth, flattened_predictions, average='macro')\n",
    "    f1 = f1_score(flattened_ground_truth, flattened_predictions, average='macro')\n",
    "    micro_precision = precision_score(flattened_ground_truth, flattened_predictions, average='micro')\n",
    "    micro_recall = recall_score(flattened_ground_truth, flattened_predictions, average='micro')\n",
    "    micro_f1 = f1_score(flattened_ground_truth, flattened_predictions, average='micro')\n",
    "    map_score = average_precision_score(relevance, [1] * len(relevance))\n",
    "    return precision, recall, f1, micro_precision, micro_recall, micro_f1, map_score\n",
    "\n",
    "knn_metrics = calculate_metrics(ground_truth, knn_predictions)\n",
    "faiss_metrics = calculate_metrics(ground_truth, faiss_predictions)\n",
    "\n",
    "# Print results\n",
    "print(\"KNN Results:\")\n",
    "for query, indices, distances in zip(queries, knn_indices_list, knn_distances_list):\n",
    "    print(f\"Query: {query}\")\n",
    "    for idx, dist in zip(indices, distances):\n",
    "        print(f\"Document ID: {sentence_to_doc_map[idx]}, Sentence: {all_sentences[idx]}, Distance: {dist}\")\n",
    "\n",
    "print(f\"KNN Metrics - Macro Precision: {knn_metrics[0]}, Macro Recall: {knn_metrics[1]}, Macro F1-Score: {knn_metrics[2]}, Micro Precision: {knn_metrics[3]}, Micro Recall: {knn_metrics[4]}, Micro F1-Score: {knn_metrics[5]}, mAP: {knn_metrics[6]}\")\n",
    "\n",
    "print(\"FAISS Results:\")\n",
    "for query, indices, distances in zip(queries, faiss_indices_list, faiss_distances_list):\n",
    "    print(f\"Query: {query}\")\n",
    "    for idx, dist in zip(indices, distances):\n",
    "        print(f\"Document ID: {sentence_to_doc_map[idx]}, Sentence: {all_sentences[idx]}, Distance: {dist}\")\n",
    "\n",
    "print(f\"FAISS Metrics - Macro Precision: {faiss_metrics[0]}, Macro Recall: {faiss_metrics[1]}, Macro F1-Score: {faiss_metrics[2]}, Micro Precision: {faiss_metrics[3]}, Micro Recall: {faiss_metrics[4]}, Micro F1-Score: {faiss_metrics[5]}, mAP: {faiss_metrics[6]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f16804a-d676-4ec1-b8f6-83d50cf2d885",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
