{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3170652fed794e84ac8caff0fd3ca3cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "27259830"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import datasets\n",
    "\n",
    "\n",
    "# R_NUMBER_SEED = 1234567 # Replace this with your own student number\n",
    "R_NUMBER_SEED = 928036 # my student number \n",
    "DOCS_TO_ADD = 1000\n",
    "query_documents = datasets.load_dataset(\"parquet\", data_files=\"./acl_anthology_queries.parquet\")[\"train\"]\n",
    "all_documents = datasets.load_dataset(\"parquet\", data_files=\"./acl_anthology_full.parquet\")[\"train\"]\n",
    "# Shuffle with seed and take only n docs\n",
    "shuffled_documents = all_documents.shuffle(seed=R_NUMBER_SEED)\n",
    "random_documents = shuffled_documents.select(range(DOCS_TO_ADD))\n",
    "# Concatenate relevant documents with random sample and shuffle again\n",
    "anthology_sample = datasets.concatenate_datasets([query_documents, random_documents]).shuffle(seed=R_NUMBER_SEED)\n",
    "# Export to Parquet to avoid downloading full anthology\n",
    "anthology_sample.to_parquet(\"./anthology_sample.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "queries = json.load(open(\"./acl_anthology_queries.json\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sumy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc: 10\n",
      "doc: 20\n",
      "doc: 30\n",
      "doc: 40\n",
      "doc: 50\n",
      "doc: 60\n",
      "doc: 70\n",
      "doc: 80\n",
      "doc: 90\n",
      "doc: 100\n",
      "doc: 110\n",
      "doc: 120\n",
      "doc: 130\n",
      "doc: 140\n",
      "doc: 150\n",
      "doc: 160\n",
      "doc: 170\n",
      "doc: 180\n",
      "doc: 190\n",
      "doc: 200\n",
      "doc: 210\n",
      "doc: 220\n",
      "doc: 230\n",
      "doc: 240\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m         count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m all_sentences,all_sentences_to_doc_map\n\u001b[0;32m---> 34\u001b[0m all_sentences, all_sentences_to_doc_map \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43manthology_sample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m spacy_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpacy extract time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspacy_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 27\u001b[0m, in \u001b[0;36mpreprocess_documents\u001b[0;34m(documents)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoc:\u001b[39m\u001b[38;5;124m\"\u001b[39m,count)        \u001b[38;5;66;03m# print(doc[\"acl_id\"])\u001b[39;00m\n\u001b[1;32m     26\u001b[0m full_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfull_text\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 27\u001b[0m spacy_doc \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m spacy_doc\u001b[38;5;241m.\u001b[39msents:\n\u001b[1;32m     29\u001b[0m     all_sentences\u001b[38;5;241m.\u001b[39mappend(sent\u001b[38;5;241m.\u001b[39mtext)\n",
      "File \u001b[0;32m~/virt/py310/lib/python3.10/site-packages/spacy/language.py:1049\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     error_handler \u001b[38;5;241m=\u001b[39m proc\u001b[38;5;241m.\u001b[39mget_error_handler()\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1049\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mproc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcomponent_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1051\u001b[0m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE109\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/virt/py310/lib/python3.10/site-packages/spacy/pipeline/trainable_pipe.pyx:52\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/virt/py310/lib/python3.10/site-packages/spacy/pipeline/tok2vec.py:126\u001b[0m, in \u001b[0;36mTok2Vec.predict\u001b[0;34m(self, docs)\u001b[0m\n\u001b[1;32m    124\u001b[0m     width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mget_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnO\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39malloc((\u001b[38;5;241m0\u001b[39m, width)) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs]\n\u001b[0;32m--> 126\u001b[0m tokvecs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokvecs\n",
      "File \u001b[0;32m~/virt/py310/lib/python3.10/site-packages/thinc/model.py:334\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OutT:\n\u001b[1;32m    331\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;124;03m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/virt/py310/lib/python3.10/site-packages/thinc/layers/chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[0;32m~/virt/py310/lib/python3.10/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/virt/py310/lib/python3.10/site-packages/thinc/layers/chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[0;32m~/virt/py310/lib/python3.10/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/virt/py310/lib/python3.10/site-packages/thinc/layers/with_array.py:36\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, Xseq, is_train)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m     33\u001b[0m     model: Model[SeqT, SeqT], Xseq: SeqT, is_train: \u001b[38;5;28mbool\u001b[39m\n\u001b[1;32m     34\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[SeqT, Callable]:\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(Xseq, Ragged):\n\u001b[0;32m---> 36\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m cast(Tuple[SeqT, Callable], \u001b[43m_ragged_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(Xseq, Padded):\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m cast(Tuple[SeqT, Callable], _padded_forward(model, Xseq, is_train))\n",
      "File \u001b[0;32m~/virt/py310/lib/python3.10/site-packages/thinc/layers/with_array.py:91\u001b[0m, in \u001b[0;36m_ragged_forward\u001b[0;34m(model, Xr, is_train)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_ragged_forward\u001b[39m(\n\u001b[1;32m     88\u001b[0m     model: Model[SeqT, SeqT], Xr: Ragged, is_train: \u001b[38;5;28mbool\u001b[39m\n\u001b[1;32m     89\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Ragged, Callable]:\n\u001b[1;32m     90\u001b[0m     layer: Model[ArrayXd, ArrayXd] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 91\u001b[0m     Y, get_dX \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataXd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackprop\u001b[39m(dYr: Ragged) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Ragged:\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m Ragged(get_dX(dYr\u001b[38;5;241m.\u001b[39mdataXd), dYr\u001b[38;5;241m.\u001b[39mlengths)\n",
      "File \u001b[0;32m~/virt/py310/lib/python3.10/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/virt/py310/lib/python3.10/site-packages/thinc/layers/concatenate.py:57\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(model: Model[InT, OutT], X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m---> 57\u001b[0m     Ys, callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m[layer(X, is_train\u001b[38;5;241m=\u001b[39mis_train) \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers])\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(Ys[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m     59\u001b[0m         data_l, backprop \u001b[38;5;241m=\u001b[39m _list_forward(model, X, Ys, callbacks, is_train)\n",
      "File \u001b[0;32m~/virt/py310/lib/python3.10/site-packages/thinc/layers/concatenate.py:57\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(model: Model[InT, OutT], X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m---> 57\u001b[0m     Ys, callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m[\u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers])\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(Ys[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m     59\u001b[0m         data_l, backprop \u001b[38;5;241m=\u001b[39m _list_forward(model, X, Ys, callbacks, is_train)\n",
      "File \u001b[0;32m~/virt/py310/lib/python3.10/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/virt/py310/lib/python3.10/site-packages/thinc/layers/chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[0;32m~/virt/py310/lib/python3.10/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/virt/py310/lib/python3.10/site-packages/thinc/layers/hashembed.py:72\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, ids, is_train)\u001b[0m\n\u001b[1;32m     70\u001b[0m seed: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mattrs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     71\u001b[0m keys \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mhash(ids, seed) \u001b[38;5;241m%\u001b[39m nV\n\u001b[0;32m---> 72\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather_add\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m drop_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_train:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### COMMENT FOR NOW, while testing, we will use it in final version\n",
    "\n",
    "# import spacy\n",
    "# from spacy.pipeline import Sentencizer\n",
    "# import time\n",
    "# # Load the SpaCy model\n",
    "# # nlp = spacy.load(\"en_core_web_sm\") ### SLOW\n",
    "\n",
    "\n",
    "# # nlp = spacy.load(\"en_core_web_sm\", disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\", \"ner\"])\n",
    "# # nlp.enable_pipe(\"senter\")\n",
    "# # nlp.enable_pipe(\"parser\")\n",
    "# # for doc in nlp.pipe(texts, n_process=4):\n",
    "# # Load SpaCy model\n",
    "# nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\", \"ner\", \"lemmatizer\"])\n",
    "# # Add the sentencizer to the pipeline\n",
    "# if not nlp.has_pipe(\"sentencizer\"):\n",
    "#     nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "# start_time = time.time()\n",
    "# def preprocess_documents(documents):\n",
    "#     all_sentences = []\n",
    "#     all_sentences_to_doc_map = []\n",
    "#     count = 1\n",
    "#     for doc in documents:\n",
    "#         if count % 10 == 0:\n",
    "#             print(\"doc:\",count)        # print(doc[\"acl_id\"])\n",
    "#         full_text = f\"{doc.get('full_text')}\"\n",
    "#         spacy_doc = nlp(full_text)\n",
    "#         for sent in spacy_doc.sents:\n",
    "#             all_sentences.append(sent.text)\n",
    "#             all_sentences_to_doc_map.append(doc[\"acl_id\"])\n",
    "#         count += 1\n",
    "#     return all_sentences,all_sentences_to_doc_map\n",
    "\n",
    "# all_sentences, all_sentences_to_doc_map = preprocess_documents(anthology_sample)\n",
    "# spacy_time = time.time() - start_time\n",
    "# print(f\"Spacy extract time: {spacy_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/marko/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc: 10\n",
      "doc: 20\n",
      "doc: 30\n",
      "doc: 40\n",
      "doc: 50\n",
      "doc: 60\n",
      "doc: 70\n",
      "doc: 80\n",
      "doc: 90\n",
      "doc: 100\n",
      "doc: 110\n",
      "doc: 120\n",
      "doc: 130\n",
      "doc: 140\n",
      "doc: 150\n",
      "doc: 160\n",
      "doc: 170\n",
      "doc: 180\n",
      "doc: 190\n",
      "doc: 200\n",
      "doc: 210\n",
      "doc: 220\n",
      "doc: 230\n",
      "doc: 240\n",
      "doc: 250\n",
      "doc: 260\n",
      "doc: 270\n",
      "doc: 280\n",
      "doc: 290\n",
      "doc: 300\n",
      "doc: 310\n",
      "doc: 320\n",
      "doc: 330\n",
      "doc: 340\n",
      "doc: 350\n",
      "doc: 360\n",
      "doc: 370\n",
      "doc: 380\n",
      "doc: 390\n",
      "doc: 400\n",
      "doc: 410\n",
      "doc: 420\n",
      "doc: 430\n",
      "doc: 440\n",
      "doc: 450\n",
      "doc: 460\n",
      "doc: 470\n",
      "doc: 480\n",
      "doc: 490\n",
      "doc: 500\n",
      "doc: 510\n",
      "doc: 520\n",
      "doc: 530\n",
      "doc: 540\n",
      "doc: 550\n",
      "doc: 560\n",
      "doc: 570\n",
      "doc: 580\n",
      "doc: 590\n",
      "doc: 600\n",
      "doc: 610\n",
      "doc: 620\n",
      "doc: 630\n",
      "doc: 640\n",
      "doc: 650\n",
      "doc: 660\n",
      "doc: 670\n",
      "doc: 680\n",
      "doc: 690\n",
      "doc: 700\n",
      "doc: 710\n",
      "doc: 720\n",
      "doc: 730\n",
      "doc: 740\n",
      "doc: 750\n",
      "doc: 760\n",
      "doc: 770\n",
      "doc: 780\n",
      "doc: 790\n",
      "doc: 800\n",
      "doc: 810\n",
      "doc: 820\n",
      "doc: 830\n",
      "doc: 840\n",
      "doc: 850\n",
      "doc: 860\n",
      "doc: 870\n",
      "doc: 880\n",
      "doc: 890\n",
      "doc: 900\n",
      "doc: 910\n",
      "doc: 920\n",
      "doc: 930\n",
      "doc: 940\n",
      "doc: 950\n",
      "doc: 960\n",
      "doc: 970\n",
      "doc: 980\n",
      "doc: 990\n",
      "doc: 1000\n",
      "doc: 1010\n",
      "doc: 1020\n",
      "doc: 1030\n",
      "doc: 1040\n",
      "doc: 1050\n",
      "doc: 1060\n",
      "doc: 1070\n",
      "doc: 1080\n",
      "doc: 1090\n",
      "doc: 1100\n",
      "doc: 1110\n",
      "doc: 1120\n",
      "doc: 1130\n",
      "doc: 1140\n",
      "doc: 1150\n",
      "NLTK extract time: 3.218236207962036 seconds\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "start_time = time.time()\n",
    "def preprocess_documents_nltk(documents):\n",
    "    all_sentences = []\n",
    "    all_sentences_to_doc_map = []\n",
    "    count = 1\n",
    "    for doc in documents:\n",
    "        if count % 10 == 0:\n",
    "            print(\"doc:\",count)\n",
    "        # print(doc[\"acl_id\"])\n",
    "        # full_text = f\"{doc.get('full_text')}\"\n",
    "        full_text = \"\"\n",
    "        if doc[\"title\"] is not None: full_text = f'{doc[\"title\"]}.' \n",
    "        if doc[\"abstract\"] is not None: full_text = f'{full_text} {doc[\"abstract\"]}.' \n",
    "        if doc[\"full_text\"] is not None: full_text = f'{full_text} {doc[\"full_text\"]}.' \n",
    "        sentences = sent_tokenize(full_text)\n",
    "        for sentence in sentences:\n",
    "            all_sentences.append(sentence)\n",
    "            all_sentences_to_doc_map.append(doc[\"acl_id\"])\n",
    "        count += 1\n",
    "    return all_sentences,all_sentences_to_doc_map\n",
    "\n",
    "preprocessed_documents, preprocessed_documents_to_doc_map = preprocess_documents_nltk(anthology_sample)\n",
    "nltk_time = time.time() - start_time\n",
    "print(f\"NLTK extract time: {nltk_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize lists to store embeddings\n",
    "# Initialize the MiniLM model for document embeddings\n",
    "minilm_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "# Function to get document embedding using MiniLM\n",
    "def get_document_embedding(document):\n",
    "    return minilm_model.encode(document, convert_to_tensor=False)\n",
    "\n",
    "# minilm_embeddings = []\n",
    "# bert_embeddings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute embeddings for the first 1000 documents\n",
    "# count = 1\n",
    "# for document in preprocessed_documents[:]:\n",
    "#     if count % 100 == 0:\n",
    "#         print(\"Processing doc:\",count)\n",
    "#     # Ensure the document is a string\n",
    "#     document = str(document)\n",
    "#     # print(\"Processing sentence embeddings:\",count)\n",
    "#     minilm_embeddings.append(get_document_embedding(document))\n",
    "#     count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK extract time: 4115.236174106598 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "minilm_embeddings = minilm_model.encode(preprocessed_documents)\n",
    "minilm_embeddings_time = time.time() - start_time\n",
    "print(f\"NLTK extract time: {minilm_embeddings_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessed_documents = preprocess_documents(anthology_sample)\n",
    "# minilm_embeddings2 = [get_mpnet_embedding(doc) for doc in preprocessed_documents[:10]]  # Only first 1000 for k-NN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "# knn2 = NearestNeighbors(n_neighbors=5, metric='cosine').fit(minilm_embeddings)\n",
    "\n",
    "# Fit NearestNeighbors model\n",
    "nn_minilm = NearestNeighbors(n_neighbors=5, metric='cosine', algorithm='brute')\n",
    "nn_minilm.fit(minilm_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embedding for the query sentence\n",
    "test_query = queries[\"queries\"][50]['q']\n",
    "test_query_sen_embed = get_document_embedding(test_query).reshape(1, -1)\n",
    "# get_nearest_neighbors(test_query_sen_embed,nn_minilm,1)\n",
    "# get_document_embedding\n",
    "\n",
    "distances, indices = nn_minilm.kneighbors(test_query_sen_embed)\n",
    "\n",
    "# Print the indices and distances of the nearest neighbors\n",
    "print(\"Indices of nearest neighbors:\", indices)\n",
    "print(\"Distances to nearest neighbors:\", distances)\n",
    "\n",
    "# Print the nearest neighbor sentences\n",
    "nearest_neighbors = [preprocessed_documents[idx] for idx in indices[0]]\n",
    "# print(\"Nearest neighbor sentences:\", nearest_neighbors)\n",
    "# query_embedding2 = get_mpnet_embedding(test_query).reshape(1, -1)\n",
    "# distances, indices = knn2.kneighbors(query_embedding2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BELOW Cell is VERY IMPORTANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Above results tell us following:\n",
    "print(\"Ground truth question:\", queries[\"queries\"][50]['q'])\n",
    "print(\"Ground truth answer:\", queries[\"queries\"][50]['a'])\n",
    "print(\"Ground truth references:\", queries[\"queries\"][50]['r'])\n",
    "print(\"OUR references\", [anthology_sample[idx][\"acl_id\"] for idx in indices])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "## Retrieving the query\n",
    "# Function to retrieve the ground truth for a given query\n",
    "def get_ground_truth(query):\n",
    "    for q in queries[\"queries\"]:\n",
    "        if q[\"q\"] == query:\n",
    "            # return q[\"r\"]\n",
    "            return [r for r in q[\"r\"]]\n",
    "    return None\n",
    "# ground_truth = get_ground_truth(query)\n",
    "\n",
    "# Function to compare the result with the ground truth for a single query\n",
    "# Function to calculate average precision for a single query\n",
    "def average_precision(retrieved_docs, ground_truth_ids):\n",
    "    if not ground_truth_ids:\n",
    "        return 0\n",
    "    retrieved_docs_set = set(retrieved_docs)\n",
    "    ground_truth_set = set(ground_truth_ids)\n",
    "    \n",
    "    num_relevant = 0\n",
    "    precision_sum = 0\n",
    "    for i, doc in enumerate(retrieved_docs):\n",
    "        if doc in ground_truth_set:\n",
    "            num_relevant += 1\n",
    "            precision_sum += num_relevant / (i + 1)\n",
    "    \n",
    "    return precision_sum / len(ground_truth_set)\n",
    "\n",
    "# Function to compare the result with the ground truth for a single query\n",
    "def compare_with_ground_truth(query_text, nn_model, dataset, ground_truth_function, k):\n",
    "    # print(\"cgt k=\",k)\n",
    "    # Get nearest neighbors\n",
    "    indices = get_nearest_neighbors(query_text, nn_model, k)\n",
    "    \n",
    "    # Convert numpy.int64 to Python int\n",
    "    indices = [int(i) for i in indices]\n",
    "    \n",
    "    # Retrieve document IDs for nearest neighbors\n",
    "    retrieved_docs = [dataset[i]['acl_id'] for i in indices]\n",
    "    \n",
    "    # Get ground truth\n",
    "    ground_truth_ids = ground_truth_function(query_text)\n",
    "    \n",
    "    if not ground_truth_ids:\n",
    "        return 0, 0, 0, 0, 0, 0, 0  # Return zeros if no ground truth is available\n",
    "    \n",
    "    # Calculate true positives, false positives, and false negatives\n",
    "    tp = len(set(retrieved_docs) & set(ground_truth_ids))\n",
    "    fp = len(retrieved_docs) - tp\n",
    "    fn = len(ground_truth_ids) - tp\n",
    "    \n",
    "    # Calculate precision, recall, and F1-score\n",
    "    precision = tp / len(retrieved_docs) if retrieved_docs else 0\n",
    "    recall = tp / len(ground_truth_ids) if ground_truth_ids else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) else 0\n",
    "    \n",
    "    # Calculate average precision\n",
    "    ap = average_precision(retrieved_docs, ground_truth_ids)\n",
    "    \n",
    "    return precision, recall, f1, ap, tp, fp, fn\n",
    "\n",
    "# Function to evaluate the model on all queries\n",
    "def evaluate_model_on_all_queries(queries, embeddings, dataset, ground_truth_function, k):\n",
    "    # Fit NearestNeighbors model\n",
    "    print(\"k=\",k)\n",
    "    nn = NearestNeighbors(n_neighbors=k, metric='cosine', algorithm='brute')\n",
    "    nn.fit(embeddings)\n",
    "\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "    ap_scores = []\n",
    "    total_tp = total_fp = total_fn = 0\n",
    "\n",
    "    for query in queries[\"queries\"]:\n",
    "        query_text = query[\"q\"]\n",
    "        precision, recall, f1, ap, tp, fp, fn = compare_with_ground_truth(query_text, nn, dataset, ground_truth_function, k)\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "        ap_scores.append(ap)\n",
    "        total_tp += tp\n",
    "        total_fp += fp\n",
    "        total_fn += fn\n",
    "    \n",
    "    # Calculate macro average metrics\n",
    "    macro_precision = np.mean(precision_scores)\n",
    "    macro_recall = np.mean(recall_scores)\n",
    "    macro_f1 = np.mean(f1_scores)\n",
    "    \n",
    "    # Calculate micro average metrics\n",
    "    micro_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) else 0\n",
    "    micro_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) else 0\n",
    "    micro_f1 = 2 * (micro_precision * micro_recall) / (micro_precision + micro_recall) if (micro_precision + micro_recall) else 0\n",
    "    \n",
    "    # Calculate mean AP\n",
    "    mean_ap = np.mean(ap_scores)\n",
    "    \n",
    "    return macro_precision, macro_recall, macro_f1, micro_precision, micro_recall, micro_f1, mean_ap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nearest_neighbors(query_text, nn_model, k):\n",
    "    # query_embedding = get_aggregated_word_embeddings([query_text])\n",
    "    # print(query_text)\n",
    "    # test_query_word_embed = get_aggregated_word_embeddings(query_text).reshape(1, -1)\n",
    "    test_query_word_embed = get_document_embedding(query_text).reshape(1, -1)\n",
    "    distances, indices = nn_model.kneighbors(test_query_word_embed, n_neighbors=k)\n",
    "    return indices[0]\n",
    "\n",
    "def nearest_neighbour(embeddings): # can be tfidf_matrix or LSI_matrix doesnt\n",
    "    nn = NearestNeighbors(n_neighbors=1, metric='cosine',algorithm = 'brute')\n",
    "    nn.fit(embeddings)\n",
    "    \n",
    "# test_query = queries[\"queries\"][0]['q']\n",
    "# get_nearest_neighbors(test_query,nn_minilm,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESULTS when summarizing preprocessing text using ntlk library and minilm_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2') for sentence embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "macro_precision, macro_recall, macro_f1, micro_precision, micro_recall, micro_f1, mean_ap = evaluate_model_on_all_queries(queries, minilm_embeddings, anthology_sample, get_ground_truth, k)\n",
    "print(f\"Macro Average Precision: {macro_precision:.4f}\")\n",
    "print(f\"Macro Average Recall: {macro_recall:.4f}\")\n",
    "print(f\"Macro Average F1-Score: {macro_f1:.4f}\")\n",
    "print(f\"Micro Average Precision: {micro_precision:.4f}\")\n",
    "print(f\"Micro Average Recall: {micro_recall:.4f}\")\n",
    "print(f\"Micro Average F1-Score: {micro_f1:.4f}\")\n",
    "print(f\"Mean Average Precision (mAP): {mean_ap:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install chromadb openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "\n",
    "CHROMA_DATA_PATH = \"chroma_data/\"\n",
    "EMBED_MODEL = \"all-MiniLM-L6-v2\"\n",
    "COLLECTION_NAME = \"demo_docs\"\n",
    "client = chromadb.PersistentClient(path=CHROMA_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "...     model_name=EMBED_MODEL\n",
    "... )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_texts = [f'{d[\"full_text\"]}' for d in anthology_sample]\n",
    "abstracts = [f'{d[\"abstract\"]}' for d in anthology_sample]\n",
    "ids=[f\"{i}\" for i in range(len(anthology_sample))]\n",
    "acl_ids = [f'acl_id:{d[\"acl_id\"]}' for d in anthology_sample]\n",
    "authors=[{\"author\":{d[\"author\"]}} for d in anthology_sample]\n",
    "# collection.add(documents=preprocessed_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.delete_collection(COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = client.create_collection(\n",
    "...     name=COLLECTION_NAME,\n",
    "...     embedding_function=embedding_func,\n",
    "...     metadata={\"hnsw:space\": \"cosine\"},\n",
    "... )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.add(\n",
    "...     documents=preprocessed_documents,\n",
    "...     ids=ids,\n",
    "# ...     metadatas=authors\n",
    "... )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_results = collection.query(\n",
    "...     query_texts=[\"What is the name of the research initiative creating resources for African languages?\"],\n",
    "...     n_results=5,\n",
    "... )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# These are the search results on example question (queries[\"queries\"][50]) using ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_results[\"ids\"][0]\n",
    "\n",
    "for id in query_results[\"ids\"][0]:\n",
    "    print(anthology_sample[int(id)][\"acl_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here are our results for the same question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"OUR references\", [anthology_sample[idx][\"acl_id\"] for idx in indices])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ground truth for the same query (q50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ground truth question:\", queries[\"queries\"][50]['q'])\n",
    "print(\"Ground truth answer:\", queries[\"queries\"][50]['a'])\n",
    "print(\"Ground truth references:\", queries[\"queries\"][50]['r'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion: It seems that our sentence transformer provides closer matches to the ground truth than results from ChromaDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
